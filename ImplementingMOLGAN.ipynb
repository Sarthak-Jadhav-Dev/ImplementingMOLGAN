{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMD0Su9Hw3SD8662sUDoj8p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sarthak-Jadhav-Dev/ImplementingMOLGAN/blob/main/ImplementingMOLGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explaination of Entire Notebook as of (12:21 pm) on 14/09/2025 is in this Link** :  https://www.perplexity.ai/search/please-explain-what-we-are-doi-OUwOPGmCQouW.7TSLQm6iw\n",
        "\n",
        "**Explaination of Entire Notebook as of (4:51 pm) on 16/09/2025 is in this Link** :  https://www.perplexity.ai/search/explain-me-this-entire-noteboo-ODtCogBhS_KVo1b6h2KjpA\n",
        "\n",
        "**Explaination of Entire Notebook as of (3:51pm) as on17/09/2025 is in this link** : https://www.perplexity.ai/search/explain-me-this-notebook-s-las-a2_bSCz9RxiPVR.1Dz57pQ\n",
        "\n",
        "**Guide For the Project is Below :** https://chatgpt.com/share/68c65ed1-0ba0-8008-9542-a798c0d211a7 (intial) and https://chatgpt.com/c/68c65f82-5890-8322-95d2-0a06d8769372 (final)\n",
        "\n",
        "\n",
        "## Structure of Notebook ##\n",
        "**The Notebook has 2 Parts**\n",
        "\n",
        "**1: Just Testing the Dependences and Environment**\n",
        "```\n",
        "Cell 1,2,3,4,5,6,7,8\n",
        "```\n",
        "\n",
        "**2: The Actual Part Where we Want to Excecute the System like Training and Testing**\n",
        "```\n",
        "Cell 1,2,9,10,11,12\n",
        "```"
      ],
      "metadata": {
        "id": "zQVgJe_1_i11"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPn2lJsO2OKn",
        "outputId": "4c2e7e8c-db6f-4d76-86a0-e811bf898066"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping torch-scatter as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-sparse as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-geometric as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in links: https://data.pyg.org/whl/torch-2.1.0+cu118.html\n",
            "Collecting torch-scatter\n",
            "  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-sparse\n",
            "  Downloading torch_sparse-0.6.18.tar.gz (209 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch-sparse) (1.16.1)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy->torch-sparse) (2.0.2)\n",
            "Building wheels for collected packages: torch-scatter, torch-sparse\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 423, in run\n",
            "    _, build_failures = build(\n",
            "                        ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/wheel_builder.py\", line 319, in build\n",
            "    wheel_file = _build_one(\n",
            "                 ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/wheel_builder.py\", line 193, in _build_one\n",
            "    wheel_path = _build_one_inside_env(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/wheel_builder.py\", line 240, in _build_one_inside_env\n",
            "    wheel_path = build_wheel_legacy(\n",
            "                 ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/build/wheel_legacy.py\", line 83, in build_wheel_legacy\n",
            "    output = call_subprocess(\n",
            "             ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/utils/subprocess.py\", line 151, in call_subprocess\n",
            "    line: str = proc.stdout.readline()\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 216, in exc_logging_wrapper\n",
            "    logger.debug(\"Exception information:\", exc_info=True)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1527, in debug\n",
            "    self._log(DEBUG, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1684, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1700, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1762, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1028, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/lib/python3.12/logging/handlers.py\", line 75, in emit\n",
            "    logging.FileHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1280, in emit\n",
            "    StreamHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1160, in emit\n",
            "    msg = self.format(record)\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 999, in format\n",
            "    return fmt.format(record)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/utils/logging.py\", line 112, in format\n",
            "    formatted = super().format(record)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 711, in format\n",
            "    record.exc_text = self.formatException(record.exc_info)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 661, in formatException\n",
            "    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
            "  File \"/usr/lib/python3.12/traceback.py\", line 124, in print_exception\n",
            "    te = TracebackException(type(value), value, tb, limit=limit, compact=True)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/traceback.py\", line 733, in __init__\n",
            "    self.stack = StackSummary._extract_from_extended_frame_gen(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/traceback.py\", line 418, in _extract_from_extended_frame_gen\n",
            "    for f, (lineno, end_lineno, colno, end_colno) in frame_gen:\n",
            "                                                     ^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/traceback.py\", line 355, in _walk_tb_with_full_positions\n",
            "    positions = _get_code_position(tb.tb_frame.f_code, tb.tb_lasti)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/traceback.py\", line 369, in _get_code_position\n",
            "    return next(itertools.islice(positions_gen, instruction_index // 2, None))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.12.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2025.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "# This Cell is For Just Installing the Dependencies\n",
        "#Remember that wait for 2 to 3 mins and then stop the prcess console will print correct code\n",
        "#Every time Run this Cell if you want to use the Entire Setup\n",
        "\n",
        "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "# !pip install torch-geometric torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
        "# !pip install pennylane matplotlib\n",
        "\n",
        "# Clean up old attempts\n",
        "!pip uninstall -y torch-scatter torch-sparse torch-geometric\n",
        "\n",
        "# Install stable builds compatible with Colab's PyTorch (2.1.0 + cu118)\n",
        "!pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.1.0+cu118.html\n",
        "!pip install torch-geometric\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#If Cell 1 Takes to Much time ie the wheels hang Problem then run this code\n",
        "\n",
        "!pip install torch==2.0.0+cu118 torchvision==0.15.1+cu118 torchaudio==2.0.1+cu118 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
        "!pip install pennylane matplotlib\n"
      ],
      "metadata": {
        "id": "4iuYYFC4wdSt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0fb76b7-d7d2-49ee-97ba-12fa1472f741"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==2.0.0+cu118 (from versions: 2.2.0+cu118, 2.2.1+cu118, 2.2.2+cu118, 2.3.0+cu118, 2.3.1+cu118, 2.4.0+cu118, 2.4.1+cu118, 2.5.0+cu118, 2.5.1+cu118, 2.6.0+cu118, 2.7.0+cu118, 2.7.1+cu118)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==2.0.0+cu118\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in links: https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
            "Collecting torch-scatter\n",
            "  Using cached torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-sparse\n",
            "  Using cached torch_sparse-0.6.18.tar.gz (209 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-cluster\n",
            "  Downloading torch_cluster-1.6.3.tar.gz (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-spline-conv\n",
            "  Downloading torch_spline_conv-1.2.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.12/dist-packages (2.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch-sparse) (1.16.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.12.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2025.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n",
            "Building wheels for collected packages: torch-scatter, torch-sparse, torch-cluster, torch-spline-conv\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hcanceled\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting pennylane\n",
            "  Downloading pennylane-0.42.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from pennylane) (3.5)\n",
            "Collecting rustworkx>=0.14.0 (from pennylane)\n",
            "  Downloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.8.0)\n",
            "Collecting appdirs (from pennylane)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting autoray<0.8,>=0.6.11 (from pennylane)\n",
            "  Downloading autoray-0.7.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from pennylane) (5.5.2)\n",
            "Collecting pennylane-lightning>=0.42 (from pennylane)\n",
            "  Downloading pennylane_lightning-0.42.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.32.4)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.13.3)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from pennylane) (4.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from pennylane) (25.0)\n",
            "Collecting diastatic-malt (from pennylane)\n",
            "  Downloading diastatic_malt-2.15.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Collecting scipy-openblas32>=0.3.26 (from pennylane-lightning>=0.42->pennylane)\n",
            "  Downloading scipy_openblas32-0.3.30.0.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (1.6.3)\n",
            "Requirement already satisfied: gast in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (0.6.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (3.1.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2025.8.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\n",
            "Downloading pennylane-0.42.3-py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autoray-0.7.2-py3-none-any.whl (930 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.8/930.8 kB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pennylane_lightning-0.42.0-cp312-cp312-manylinux_2_28_x86_64.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading diastatic_malt-2.15.2-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy_openblas32-0.3.30.0.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m130.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: appdirs, scipy-openblas32, rustworkx, autoray, diastatic-malt, pennylane-lightning, pennylane\n",
            "Successfully installed appdirs-1.4.4 autoray-0.7.2 diastatic-malt-2.15.2 pennylane-0.42.3 pennylane-lightning-0.42.0 rustworkx-0.17.1 scipy-openblas32-0.3.30.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Guide Link:** https://chatgpt.com/share/68c65ed1-0ba0-8008-9542-a798c0d211a7 (intial) and\n",
        "https://chatgpt.com/c/68c65f82-5890-8322-95d2-0a06d8769372 (final)\n",
        "\n",
        "**Problem with Rdkit Method Great question** 🙌\n",
        "— the RDKit issues you faced in Colab are very common. Let me break down why it was so messy:\n",
        "\n",
        "**1. Colab’s Python version changed**\n",
        "\n",
        "In 2023, Colab used Python 3.8/3.9, and rdkit-pypi or apt-get install python3-rdkit worked smoothly.\n",
        "\n",
        "In 2024–2025, Colab upgraded to Python 3.11/3.12.\n",
        "\n",
        "RDKit’s wheels (precompiled binaries) on PyPI (rdkit-pypi) do not always provide builds for the latest Python versions right away. That’s why you saw:\n",
        "\n",
        "ERROR: No matching distribution found for rdkit-pypi\n",
        "\n",
        "\n",
        "→ It simply didn’t exist for your Python version.\n",
        "\n",
        "**2. Mixing installation methods**\n",
        "\n",
        "Colab has multiple ways to install RDKit:\n",
        "\n",
        "!pip install rdkit or rdkit-pypi (PyPI wheels).\n",
        "\n",
        "!apt-get install python3-rdkit (Ubuntu system package).\n",
        "\n",
        "condacolab + mamba install -c conda-forge rdkit (Conda build).\n",
        "\n",
        "If you run more than one, Colab may end up with conflicting RDKit versions (one in /usr/lib/python3/dist-packages/, another in /usr/local/lib/python3.x/dist-packages/). This is what caused the ImportError: cannot import name 'rdBase' — Python was importing a half-installed RDKit from the wrong folder.\n",
        "\n",
        "**3. Conda pinning issue**\n",
        "\n",
        "When you used condacolab, it pinned Python=3.12 by default, while Colab itself was running Python=3.11. That mismatch is why mamba kept complaining about “pinning does not match.”\n",
        "\n",
        "✅ In short:\n",
        "\n",
        "The errors happened because Colab’s Python version is too new for some RDKit builds,\n",
        "\n",
        "and mixing pip/apt/conda installs left you with a broken hybrid installation.\n",
        "\n",
        "🔧 How to avoid this in the future\n",
        "\n",
        "Pick one install method only (pip OR conda, not both).\n",
        "\n",
        "For Colab today (Python 3.11/3.12), the most reliable is:\n",
        "\n",
        "!pip install rdkit\n",
        "\n",
        "\n",
        "Then just do:\n",
        "\n",
        "from rdkit import Chem\n",
        "print(Chem.MolFromSmiles(\"CCO\"))\n",
        "\n",
        "\n",
        "If that runs, you’re good.\n",
        "\n",
        "👉 Since your hybrid MolGAN setup (PyTorch + PennyLane + QM9 from torch-geometric) works without RDKit for now, you can move forward. Later, when you need QED/LogP/SA metrics, we can add RDKit back carefully."
      ],
      "metadata": {
        "id": "o4xn03Qo4p_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This Cell was Just For Testing Purpose that the Dependencies we installed were correct , and have the Following Data Which is Printed Below\n",
        "from torch_geometric.datasets import QM9\n",
        "dataset = QM9(root=\"./data/QM9\")\n",
        "\n",
        "print(\"QM9 size:\", len(dataset))\n",
        "print(dataset[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zdsgn-r17MKx",
        "outputId": "6c973bdb-7c52-4142-94bd-21058b4a3b54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://data.pyg.org/datasets/qm9_v3.zip\n",
            "Extracting data/QM9/raw/qm9_v3.zip\n",
            "Processing...\n",
            "Using a pre-processed version of the dataset. Please install 'rdkit' to alternatively process the raw data.\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QM9 size: 130831\n",
            "Data(x=[5, 11], edge_index=[2, 8], edge_attr=[8, 4], y=[1, 19], pos=[5, 3], idx=[1], name='gdb_1', z=[5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This the cell for Penny Lane , Only use if we Need Panylane Further\n",
        "!pip install pennylane\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23BfAox98W7r",
        "outputId": "f2901252-9432-4254-bfc5-0d6ce0f5661f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pennylane\n",
            "  Downloading pennylane-0.42.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from pennylane) (3.5)\n",
            "Collecting rustworkx>=0.14.0 (from pennylane)\n",
            "  Downloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.8.0)\n",
            "Collecting appdirs (from pennylane)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting autoray<0.8,>=0.6.11 (from pennylane)\n",
            "  Downloading autoray-0.7.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from pennylane) (5.5.2)\n",
            "Collecting pennylane-lightning>=0.42 (from pennylane)\n",
            "  Downloading pennylane_lightning-0.42.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.32.4)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.13.3)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from pennylane) (4.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from pennylane) (25.0)\n",
            "Collecting diastatic-malt (from pennylane)\n",
            "  Downloading diastatic_malt-2.15.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.0.2)\n",
            "Collecting scipy-openblas32>=0.3.26 (from pennylane-lightning>=0.42->pennylane)\n",
            "  Downloading scipy_openblas32-0.3.30.0.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: astunparse in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (1.6.3)\n",
            "Requirement already satisfied: gast in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (0.6.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (3.1.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2025.8.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\n",
            "Downloading pennylane-0.42.3-py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autoray-0.7.2-py3-none-any.whl (930 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.8/930.8 kB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pennylane_lightning-0.42.0-cp312-cp312-manylinux_2_28_x86_64.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading diastatic_malt-2.15.2-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy_openblas32-0.3.30.0.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: appdirs, scipy-openblas32, rustworkx, autoray, diastatic-malt, pennylane-lightning, pennylane\n",
            "Successfully installed appdirs-1.4.4 autoray-0.7.2 diastatic-malt-2.15.2 pennylane-0.42.3 pennylane-lightning-0.42.0 rustworkx-0.17.1 scipy-openblas32-0.3.30.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pennylane as qml\n",
        "\n",
        "n_qubits = 8\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "@qml.qnode(dev, interface=\"torch\")\n",
        "def quantum_circuit(inputs, weights):\n",
        "    # Encode noise into qubits\n",
        "    qml.AngleEmbedding(inputs, wires=range(n_qubits))\n",
        "    # Variational block\n",
        "    qml.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
        "    return qml.probs(wires=range(n_qubits))\n",
        "\n",
        "class QuantumLayer(nn.Module):\n",
        "    def __init__(self, n_layers=2):\n",
        "        super().__init__()\n",
        "        shape = qml.StronglyEntanglingLayers.shape(n_layers=n_layers, n_wires=n_qubits)\n",
        "        self.weights = nn.Parameter(torch.randn(shape))\n",
        "    def forward(self, x):\n",
        "        outputs = []\n",
        "        for xi in x:\n",
        "            probs = quantum_circuit(xi, self.weights)\n",
        "            outputs.append(probs.to(dtype=torch.float32))  # force float32\n",
        "        return torch.stack(outputs)\n",
        "\n"
      ],
      "metadata": {
        "id": "IX0P92pU72vY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, noise_dim=8, hidden_dim=64, out_dim=128):\n",
        "        super().__init__()\n",
        "        self.q_layer = QuantumLayer(n_layers=2)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(2**n_qubits, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, out_dim)\n",
        "        )\n",
        "    def forward(self, z):\n",
        "        q_out = self.q_layer(z)\n",
        "        return self.fc(q_out)\n",
        "\n",
        "class Cycle(nn.Module):\n",
        "    def __init__(self, in_dim=128, noise_dim=8):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(in_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, noise_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "G = Generator()\n",
        "C = Cycle()\n"
      ],
      "metadata": {
        "id": "v0LWR__S8hcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizerG = torch.optim.Adam(G.parameters(), lr=1e-3)\n",
        "optimizerC = torch.optim.Adam(C.parameters(), lr=1e-3)\n",
        "\n",
        "for step in range(5):\n",
        "    z = torch.randn(4, n_qubits)  # batch of noise\n",
        "    fake = G(z)\n",
        "    z_recon = C(fake)\n",
        "\n",
        "    cycle_loss = torch.nn.functional.mse_loss(z_recon, z)\n",
        "\n",
        "    optimizerG.zero_grad()\n",
        "    optimizerC.zero_grad()\n",
        "    cycle_loss.backward()\n",
        "    optimizerG.step()\n",
        "    optimizerC.step()\n",
        "\n",
        "    print(f\"Step {step}: cycle_loss = {cycle_loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMTMkmHR8mx9",
        "outputId": "67630a50-5b6a-47f5-c85a-f433aa4aba11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: cycle_loss = 1.3365\n",
            "Step 1: cycle_loss = 0.8074\n",
            "Step 2: cycle_loss = 0.7488\n",
            "Step 3: cycle_loss = 0.6074\n",
            "Step 4: cycle_loss = 0.7769\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**✅ What we have achieved so far**\n",
        "\n",
        "**Environment setup (Cell 1)**: PyTorch + PyTorch Geometric + PennyLane are working on Colab (T4 GPU ready).\n",
        "\n",
        "**Dataset (Cell 2):** QM9 dataset is accessible — we haven’t used it fully yet, but it’s ready.\n",
        "\n",
        "**Quantum Layer (Cell 3):** A variational quantum circuit (VQC, 8 qubits) built with PennyLane.\n",
        "\n",
        "**Generator + Cycle (Cell 4):** Generator maps noise → latent representation, Cycle maps back → noise.\n",
        "\n",
        "**Training (Cell 5):** Cycle-consistency loss decreased, proving gradients flow through quantum → classical → cycle.\n",
        "\n",
        "The numbers going down (1.33 → 0.80 → 0.74 → 0.60 → 0.77) mean the model is actually learning to reconstruct noise. ✅\n",
        "\n",
        "This is essentially the core hybrid quantum GAN building block from your paper — the “HQ-Cycle” idea is alive and working in a small demo."
      ],
      "metadata": {
        "id": "hTx2iBPB9U__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where you are in the roadmap\n",
        "\n",
        "**✅ Phase 1: Environment done.**\n",
        "\n",
        "**✅ Phase 2: Dataset loaded**.\n",
        "\n",
        "**✅ Phase 3: Generator + Cycle with quantum layer is working.**\n",
        "\n",
        "**🔜 Phase 4: Add Discriminator + WGAN training.**\n",
        "\n",
        "**🔜 Phase 5: Hook in QM9 graphs as real samples.**\n",
        "\n",
        "**🔜 Phase 6: Add reward & evaluation metrics.**"
      ],
      "metadata": {
        "id": "r2UOFsaY-GJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Phase 4: Discriminator + WGAN-GP training loop (single cell) =====\n",
        "# Assumes: PyTorch, torch_geometric dataset QM9 downloaded in ./data/QM9,\n",
        "# and PennyLane installed. It re-defines a compatible Generator.\n",
        "\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch_geometric.datasets import QM9\n",
        "from torch_geometric.data import Data\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ----------------- Parameters -----------------\n",
        "n_qubits = 8\n",
        "noise_dim = n_qubits                 # we use an 8-dim noise that gets encoded in the VQC\n",
        "quantum_out_dim = 2**n_qubits        # 256\n",
        "GRAPH_ADJ_FLAT = 9*9*5               # 405\n",
        "GRAPH_FEAT_FLAT = 9*5                # 45\n",
        "GRAPH_DIM = GRAPH_ADJ_FLAT + GRAPH_FEAT_FLAT  # 450\n",
        "batch_size = 16\n",
        "critic_iters = 5\n",
        "lr = 2e-4\n",
        "lambda_gp = 10.0\n",
        "# ----------------------------------------------\n",
        "\n",
        "# ----------------- Quantum Layer (PennyLane) -----------------\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as pnp\n",
        "\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "@qml.qnode(dev, interface=\"torch\")\n",
        "def qnode_probs(inputs, weights):\n",
        "    # inputs: vector length = n_qubits\n",
        "    qml.AngleEmbedding(inputs, wires=range(n_qubits))\n",
        "    qml.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
        "    return qml.probs(wires=range(n_qubits))\n",
        "\n",
        "class QuantumLayer(nn.Module):\n",
        "    def __init__(self, n_layers=2):\n",
        "        super().__init__()\n",
        "        shape = qml.StronglyEntanglingLayers.shape(n_layers=n_layers, n_wires=n_qubits)\n",
        "        # PennyLane weights are float64 by default; we keep them as torch.Parameter\n",
        "        self.weights = nn.Parameter(torch.randn(shape, dtype=torch.float64) * 0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch, n_qubits) torch.float32\n",
        "        outs = []\n",
        "        for i in range(x.shape[0]):\n",
        "            xi = x[i].double()                # qnode expects double by default\n",
        "            probs = qnode_probs(xi, self.weights)   # double tensor (2**n_qubits,)\n",
        "            outs.append(probs.to(dtype=torch.float32))\n",
        "        return torch.stack(outs, dim=0)  # (batch, 2**n_qubits)\n",
        "\n",
        "# ----------------- Generator (quantum + classical) -----------------\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, n_qubits=8, quantum_layers=2, hidden=512, out_dim=GRAPH_DIM):\n",
        "        super().__init__()\n",
        "        self.q_layer = QuantumLayer(n_layers=quantum_layers)\n",
        "        # Map quantum probs (256) -> hidden -> out_dim (450)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(quantum_out_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, out_dim),\n",
        "            nn.Tanh()   # tanh to keep outputs in -1..1 for stability; we'll map to [0,1] later if needed\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        # z: (batch, n_qubits) float32\n",
        "        q_out = self.q_layer(z)          # (batch, 256), float32\n",
        "        out = self.fc(q_out.float())     # ensure float32 going into fc\n",
        "        return out                        # (batch, GRAPH_DIM), float32\n",
        "\n",
        "# ----------------- Discriminator / Critic -----------------\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, in_dim=GRAPH_DIM, hidden=512):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(hidden, hidden//2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(hidden//2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).view(-1)\n",
        "\n",
        "# ----------------- Utility: gradient penalty -----------------\n",
        "def gradient_penalty(critic, real, fake, device):\n",
        "    batch_size = real.size(0)\n",
        "    alpha = torch.rand(batch_size, 1, device=device)\n",
        "    alpha = alpha.expand_as(real)\n",
        "    interpolates = alpha * real + ((1 - alpha) * fake)\n",
        "    interpolates.requires_grad_(True)\n",
        "    d_interpolates = critic(interpolates)\n",
        "    grads = torch.autograd.grad(\n",
        "        outputs=d_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=torch.ones_like(d_interpolates),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True\n",
        "    )[0]\n",
        "    grads = grads.view(batch_size, -1)\n",
        "    grad_norm = grads.norm(2, dim=1)\n",
        "    gp = ((grad_norm - 1.0) ** 2).mean()\n",
        "    return gp\n",
        "\n",
        "# ----------------- QM9 -> (adj_flat, feat_flat) converter -----------------\n",
        "# Try to use RDKit if available to get reliable bonds/atom types; otherwise fallback via PyG fields.\n",
        "use_rdkit = False\n",
        "try:\n",
        "    from rdkit import Chem\n",
        "    use_rdkit = True\n",
        "    print(\"RDKit available: using RDKit for graph extraction\")\n",
        "except Exception as e:\n",
        "    print(\"RDKit not available; using fallback graph extraction (edge_index / atomic numbers)\")\n",
        "\n",
        "# atom mapping\n",
        "ATOM_TYPES = ['C','O','N','F','H']\n",
        "ATOM_TO_IDX = {a:i for i,a in enumerate(ATOM_TYPES)}\n",
        "\n",
        "def data_to_graphvec(data):\n",
        "    \"\"\"\n",
        "    Input: a torch_geometric.data.Data object (QM9).\n",
        "    Output: torch tensor shape (GRAPH_DIM,) float32\n",
        "    This will produce adjacency one-hot for 5 bond types (9x9x5 -> 405) and features 9x5 -> 45\n",
        "    \"\"\"\n",
        "    # initialize\n",
        "    adj = torch.zeros((9,9,5), dtype=torch.float32)\n",
        "    feats = torch.zeros((9,5), dtype=torch.float32)\n",
        "\n",
        "    if use_rdkit and hasattr(data, 'smiles'):\n",
        "        smi = data.smiles if isinstance(data.smiles, str) else data.smiles[0]\n",
        "        mol = Chem.MolFromSmiles(smi)\n",
        "        if mol is None:\n",
        "            return None\n",
        "        # atoms\n",
        "        for i,a in enumerate(mol.GetAtoms()):\n",
        "            if i>=9: break\n",
        "            sym = a.GetSymbol()\n",
        "            if sym in ATOM_TO_IDX:\n",
        "                feats[i, ATOM_TO_IDX[sym]] = 1.0\n",
        "            else:\n",
        "                feats[i, -1] = 1.0\n",
        "        # bonds\n",
        "        for b in mol.GetBonds():\n",
        "            i = b.GetBeginAtomIdx(); j = b.GetEndAtomIdx()\n",
        "            if i<9 and j<9:\n",
        "                bt = b.GetBondType()\n",
        "                if bt == Chem.BondType.SINGLE: idx=0\n",
        "                elif bt == Chem.BondType.DOUBLE: idx=1\n",
        "                elif bt == Chem.BondType.TRIPLE: idx=2\n",
        "                elif bt == Chem.BondType.AROMATIC: idx=3\n",
        "                else: idx=4\n",
        "                adj[i,j,idx]=1.0; adj[j,i,idx]=1.0\n",
        "    else:\n",
        "        # Fallback: use atomic numbers (data.z) and edge_index as single bond\n",
        "        # data.z is (num_nodes,) atomic numbers\n",
        "        if hasattr(data, 'z'):\n",
        "            z = data.z.cpu().numpy()\n",
        "            for i, atom_num in enumerate(z):\n",
        "                if i>=9: break\n",
        "                sym = None\n",
        "                # map common atomic numbers to symbols\n",
        "                if atom_num == 6: sym='C'\n",
        "                elif atom_num == 8: sym='O'\n",
        "                elif atom_num == 7: sym='N'\n",
        "                elif atom_num == 9: sym='F'\n",
        "                elif atom_num == 1: sym='H'\n",
        "                if sym in ATOM_TO_IDX:\n",
        "                    feats[i, ATOM_TO_IDX[sym]] = 1.0\n",
        "                else:\n",
        "                    feats[i, -1] = 1.0\n",
        "        # edges: data.edge_index (2, E) — treat as single bond type index 0\n",
        "        if hasattr(data, 'edge_index'):\n",
        "            ei = data.edge_index.cpu().numpy()\n",
        "            for a,b in ei.T:\n",
        "                if a<9 and b<9:\n",
        "                    adj[a,b,0] = 1.0\n",
        "                    adj[b,a,0] = 1.0\n",
        "\n",
        "    adj_flat = adj.reshape(-1)\n",
        "    feat_flat = feats.reshape(-1)\n",
        "    vec = torch.cat([adj_flat, feat_flat], dim=0).to(dtype=torch.float32)\n",
        "    if vec.numel() != GRAPH_DIM:\n",
        "        return None\n",
        "    return vec\n",
        "\n",
        "# ----------------- Prepare a small real dataset loader (molecules with <=9 atoms) -------------\n",
        "print(\"Loading QM9 dataset (this may take time only on first download)...\")\n",
        "qm9 = QM9(root=\"./data/QM9\")\n",
        "# filter and convert to vectors\n",
        "real_graphs = []\n",
        "for i in range(len(qm9)):\n",
        "    data = qm9[i]\n",
        "    # many QM9 graphs have nodes <=9; we will only take up to 9 nodes via converter\n",
        "    vec = data_to_graphvec(data)\n",
        "    if vec is not None:\n",
        "        real_graphs.append(vec)\n",
        "    if len(real_graphs) >= 2000:   # cap for speed; increase if you want more\n",
        "        break\n",
        "\n",
        "if len(real_graphs) == 0:\n",
        "    raise RuntimeError(\"No valid real graphs extracted from QM9. If RDKit is not installed or dataset fields changed, you may need to fallback to a synthetic dataset.\")\n",
        "\n",
        "real_tensor = torch.stack(real_graphs)  # (N, GRAPH_DIM)\n",
        "print(\"Prepared real graph vectors:\", real_tensor.shape)\n",
        "\n",
        "real_loader = DataLoader(real_tensor, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "# ----------------- Instantiate models -----------------\n",
        "G = Generator(n_qubits=n_qubits, quantum_layers=2, hidden=512, out_dim=GRAPH_DIM).to(device)\n",
        "C = Critic(in_dim=GRAPH_DIM, hidden=512).to(device)\n",
        "\n",
        "opt_G = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.9))\n",
        "opt_C = optim.Adam(C.parameters(), lr=lr, betas=(0.5, 0.9))\n",
        "\n",
        "# ----------------- Training loop (WGAN-GP) -----------------\n",
        "steps = 800    # small test run; increase later\n",
        "print_every = 50\n",
        "\n",
        "real_iter = iter(real_loader)\n",
        "for step in range(steps):\n",
        "    # Train critic more times\n",
        "    for _ in range(critic_iters):\n",
        "        try:\n",
        "            real_batch = next(real_iter)\n",
        "        except StopIteration:\n",
        "            real_iter = iter(real_loader)\n",
        "            real_batch = next(real_iter)\n",
        "        real_batch = real_batch.to(device)\n",
        "\n",
        "        # sample noise and generate fake graphs\n",
        "        z = torch.randn(batch_size, n_qubits, device=device).float()\n",
        "        with torch.no_grad():\n",
        "            fake_batch = G(z)\n",
        "\n",
        "        # critic scores\n",
        "        real_scores = C(real_batch)\n",
        "        fake_scores = C(fake_batch)\n",
        "\n",
        "        # gradient penalty\n",
        "        gp = gradient_penalty(C, real_batch, fake_batch, device)\n",
        "\n",
        "        loss_C = fake_scores.mean() - real_scores.mean() + lambda_gp * gp\n",
        "\n",
        "        opt_C.zero_grad()\n",
        "        loss_C.backward()\n",
        "        opt_C.step()\n",
        "\n",
        "    # Train generator (one step)\n",
        "    z = torch.randn(batch_size, n_qubits, device=device).float()\n",
        "    fake = G(z)\n",
        "    loss_G = -C(fake).mean()   # WGAN generator loss\n",
        "\n",
        "    opt_G.zero_grad()\n",
        "    loss_G.backward()\n",
        "    opt_G.step()\n",
        "\n",
        "    if (step+1) % print_every == 0 or step==0:\n",
        "        # compute some stats\n",
        "        with torch.no_grad():\n",
        "            r = real_batch[:4].cpu().numpy()\n",
        "            f = fake[:4].cpu().numpy()\n",
        "        print(f\"Step {step+1}/{steps} | loss_C: {loss_C.item():.4f} | loss_G: {loss_G.item():.4f}\")\n",
        "\n",
        "print(\"Training finished (toy run). You can now: - increase `steps` - increase dataset size - add cycle+reward to the generator loss.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLqYbFmgt5Sb",
        "outputId": "947723cb-b71d-4b9d-c641-cfc27337d303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "RDKit not available; using fallback graph extraction (edge_index / atomic numbers)\n",
            "Loading QM9 dataset (this may take time only on first download)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://data.pyg.org/datasets/qm9_v3.zip\n",
            "Extracting data/QM9/raw/qm9_v3.zip\n",
            "Processing...\n",
            "Using a pre-processed version of the dataset. Please install 'rdkit' to alternatively process the raw data.\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared real graph vectors: torch.Size([2000, 450])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:829: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:179.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1/800 | loss_C: 6.4979 | loss_G: 0.0511\n",
            "Step 50/800 | loss_C: -4.0554 | loss_G: -0.6642\n",
            "Step 100/800 | loss_C: -3.5074 | loss_G: -1.1817\n",
            "Step 150/800 | loss_C: -3.1273 | loss_G: -0.8033\n",
            "Step 200/800 | loss_C: -2.8350 | loss_G: -0.1245\n",
            "Step 250/800 | loss_C: -2.9427 | loss_G: -0.0965\n",
            "Step 300/800 | loss_C: -3.1811 | loss_G: 0.2207\n",
            "Step 350/800 | loss_C: -3.0227 | loss_G: 0.0980\n",
            "Step 400/800 | loss_C: -3.1856 | loss_G: 0.2310\n",
            "Step 450/800 | loss_C: -2.9618 | loss_G: 0.0095\n",
            "Step 500/800 | loss_C: -3.0193 | loss_G: -0.0357\n",
            "Step 550/800 | loss_C: -2.6964 | loss_G: -0.1676\n",
            "Step 600/800 | loss_C: -2.6583 | loss_G: -0.0921\n",
            "Step 650/800 | loss_C: -2.7677 | loss_G: -0.1022\n",
            "Step 700/800 | loss_C: -2.8886 | loss_G: -0.2136\n",
            "Step 750/800 | loss_C: -2.4120 | loss_G: -0.3621\n",
            "Step 800/800 | loss_C: -2.6212 | loss_G: -0.2321\n",
            "Training finished (toy run). You can now: - increase `steps` - increase dataset size - add cycle+reward to the generator loss.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For Cell 4**\n",
        "\n",
        "What it did:\n",
        "\n",
        "Defined a Quantum Generator (G): noise → VQC → classical MLP → 450-dim graph vector.\n",
        "\n",
        "Defined a Critic (C): MLP that scores real vs fake graphs.\n",
        "\n",
        "Converted QM9 molecules into fixed-size graph vectors (adjacency + features).\n",
        "\n",
        "Implemented WGAN-GP training loop:\n",
        "\n",
        "Critic tries to separate real/fake graphs.\n",
        "\n",
        "Generator tries to fool the Critic.\n",
        "\n",
        "Printed loss_C (critic loss) and loss_G (generator loss).\n",
        "\n",
        "What you achieved:\n",
        "✅ You trained your first Hybrid Quantum GAN — generator with a quantum circuit vs critic, using real QM9 molecules as training targets."
      ],
      "metadata": {
        "id": "sgSCCCWzwVus"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cell 5 (Phase-5: Add Cycle Consistency)**\n",
        "\n",
        "What it did:\n",
        "\n",
        "Added a Cycle Network (Cyc): graph vector → reconstruct original noise.\n",
        "\n",
        "Added Cycle loss = MSE(z_recon, z).\n",
        "\n",
        "Modified generator objective:\n",
        "\n",
        "𝐿\n",
        "𝐺\n",
        "=\n",
        "−\n",
        "Critic(fake)\n",
        "+\n",
        "𝜆\n",
        "cycle\n",
        "⋅\n",
        "CycleLoss\n",
        "L\n",
        "G\n",
        "\t​\n",
        "\n",
        "=−Critic(fake)+λ\n",
        "cycle\n",
        "\t​\n",
        "\n",
        "⋅CycleLoss\n",
        "\n",
        "Training loop now updates Generator + Cycle together.\n",
        "\n",
        "Printed loss_C, loss_G, and cycle_loss.\n",
        "\n",
        "What you achieved:\n",
        "✅ You upgraded the model to HQ-Cycle-GAN.\n",
        "\n",
        "Generator learns not just to fool the critic, but also to keep its latent noise space consistent (important for stable molecule generation)."
      ],
      "metadata": {
        "id": "dexY3nZ-wdNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Phase 5: Add Cycle Consistency to GAN training =====\n",
        "# Assumes Phase-4 code already ran and G, C, opt_G, opt_C, real_loader, etc. are defined.\n",
        "\n",
        "# ----------------- Cycle Network -----------------\n",
        "class CycleNet(nn.Module):\n",
        "    def __init__(self, in_dim=GRAPH_DIM, out_dim=noise_dim, hidden=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, out_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "Cyc = CycleNet(in_dim=GRAPH_DIM, out_dim=noise_dim).to(device)\n",
        "opt_Cyc = optim.Adam(Cyc.parameters(), lr=lr)\n",
        "\n",
        "lambda_cycle = 10.0   # weight for cycle loss\n",
        "\n",
        "# ----------------- Training loop with Cycle Loss -----------------\n",
        "steps = 500\n",
        "print_every = 50\n",
        "\n",
        "real_iter = iter(real_loader)\n",
        "for step in range(steps):\n",
        "    # ---- Train critic ----\n",
        "    for _ in range(critic_iters):\n",
        "        try:\n",
        "            real_batch = next(real_iter)\n",
        "        except StopIteration:\n",
        "            real_iter = iter(real_loader)\n",
        "            real_batch = next(real_iter)\n",
        "        real_batch = real_batch.to(device)\n",
        "\n",
        "        z = torch.randn(batch_size, noise_dim, device=device).float()\n",
        "        with torch.no_grad():\n",
        "            fake_batch = G(z)\n",
        "\n",
        "        real_scores = C(real_batch)\n",
        "        fake_scores = C(fake_batch)\n",
        "        gp = gradient_penalty(C, real_batch, fake_batch, device)\n",
        "\n",
        "        loss_C = fake_scores.mean() - real_scores.mean() + lambda_gp * gp\n",
        "\n",
        "        opt_C.zero_grad()\n",
        "        loss_C.backward()\n",
        "        opt_C.step()\n",
        "\n",
        "    # ---- Train generator + cycle ----\n",
        "    z = torch.randn(batch_size, noise_dim, device=device).float()\n",
        "    fake = G(z)\n",
        "\n",
        "    # cycle consistency\n",
        "    z_recon = Cyc(fake)\n",
        "    cycle_loss = F.mse_loss(z_recon, z)\n",
        "\n",
        "    # generator loss (WGAN + cycle)\n",
        "    loss_G = -C(fake).mean() + lambda_cycle * cycle_loss\n",
        "\n",
        "    opt_G.zero_grad()\n",
        "    opt_Cyc.zero_grad()\n",
        "    loss_G.backward()\n",
        "    opt_G.step()\n",
        "    opt_Cyc.step()\n",
        "\n",
        "    if (step+1) % print_every == 0 or step==0:\n",
        "        print(f\"Step {step+1}/{steps} | loss_C: {loss_C.item():.4f} | loss_G: {loss_G.item():.4f} | cycle_loss: {cycle_loss.item():.4f}\")\n",
        "\n",
        "print(\"Phase 5 training finished ✅ (GAN + Cycle).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qp6vzZQhofXJ",
        "outputId": "bf1dbe87-cb42-404b-ba9e-bbf74fc82518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1/500 | loss_C: -2.5160 | loss_G: 9.0162 | cycle_loss: 0.9345\n",
            "Step 50/500 | loss_C: -2.5529 | loss_G: 8.3670 | cycle_loss: 0.8638\n",
            "Step 100/500 | loss_C: -2.4494 | loss_G: 10.8148 | cycle_loss: 1.0981\n",
            "Step 150/500 | loss_C: -2.5385 | loss_G: 9.5043 | cycle_loss: 0.9711\n",
            "Step 200/500 | loss_C: -2.3893 | loss_G: 9.1311 | cycle_loss: 0.9290\n",
            "Step 250/500 | loss_C: -2.1534 | loss_G: 10.0306 | cycle_loss: 1.0356\n",
            "Step 300/500 | loss_C: -2.2409 | loss_G: 9.3586 | cycle_loss: 0.9608\n",
            "Step 350/500 | loss_C: -2.2464 | loss_G: 8.7382 | cycle_loss: 0.8948\n",
            "Step 400/500 | loss_C: -2.1547 | loss_G: 9.3212 | cycle_loss: 0.9718\n",
            "Step 450/500 | loss_C: -2.1129 | loss_G: 6.4219 | cycle_loss: 0.6667\n",
            "Step 500/500 | loss_C: -2.0063 | loss_G: 9.3943 | cycle_loss: 0.9812\n",
            "Phase 5 training finished ✅ (GAN + Cycle).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The original paper’s HQ-Cycle-MolGAN has 3 parts in the objective:\n",
        "\n",
        "WGAN loss → adversarial training (✅ done in Cell 4).\n",
        "\n",
        "Cycle loss → consistency between latent space and molecules (✅ done in Cell 5).\n",
        "\n",
        "Reward loss → optimize molecules for chemical properties like QED, LogP, SA (❌ not yet added)."
      ],
      "metadata": {
        "id": "-qrHRC2pwojI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Phase 6: Reward Integration without RDKit =====\n",
        "# Assumes Phase-5 (CycleNet, G, C, etc.) has already run.\n",
        "\n",
        "def compute_synthetic_reward(fake_batch):\n",
        "    \"\"\"\n",
        "    Fake reward signal as a placeholder for RDKit properties.\n",
        "    Measures variance of adjacency part of graph vector.\n",
        "    Higher variance => higher reward.\n",
        "    \"\"\"\n",
        "    adj = fake_batch[:, :GRAPH_ADJ_FLAT]   # first 405 dims = adjacency\n",
        "    reward = adj.var(dim=1)                # variance across edges\n",
        "    reward = torch.sigmoid(reward)         # squash into (0,1)\n",
        "    return reward\n",
        "\n",
        "gamma_reward = 5.0\n",
        "steps = 300\n",
        "print_every = 50\n",
        "\n",
        "real_iter = iter(real_loader)\n",
        "for step in range(steps):\n",
        "    # ---- Train Critic ----\n",
        "    for _ in range(critic_iters):\n",
        "        try:\n",
        "            real_batch = next(real_iter)\n",
        "        except StopIteration:\n",
        "            real_iter = iter(real_loader)\n",
        "            real_batch = next(real_iter)\n",
        "        real_batch = real_batch.to(device)\n",
        "\n",
        "        z = torch.randn(batch_size, noise_dim, device=device).float()\n",
        "        with torch.no_grad():\n",
        "            fake_batch = G(z)\n",
        "\n",
        "        real_scores = C(real_batch)\n",
        "        fake_scores = C(fake_batch)\n",
        "        gp = gradient_penalty(C, real_batch, fake_batch, device)\n",
        "\n",
        "        loss_C = fake_scores.mean() - real_scores.mean() + lambda_gp * gp\n",
        "        opt_C.zero_grad()\n",
        "        loss_C.backward()\n",
        "        opt_C.step()\n",
        "\n",
        "    # ---- Train Generator + Cycle + Reward ----\n",
        "    z = torch.randn(batch_size, noise_dim, device=device).float()\n",
        "    fake = G(z)\n",
        "\n",
        "    # cycle loss\n",
        "    z_recon = Cyc(fake)\n",
        "    cycle_loss = F.mse_loss(z_recon, z)\n",
        "\n",
        "    # synthetic reward\n",
        "    reward_vals = compute_synthetic_reward(fake)\n",
        "    reward_loss = -reward_vals.mean()   # minus: maximize reward\n",
        "\n",
        "    # total generator loss\n",
        "    loss_G = -C(fake).mean() + lambda_cycle * cycle_loss + gamma_reward * reward_loss\n",
        "\n",
        "    opt_G.zero_grad()\n",
        "    opt_Cyc.zero_grad()\n",
        "    loss_G.backward()\n",
        "    opt_G.step()\n",
        "    opt_Cyc.step()\n",
        "\n",
        "    if (step+1) % print_every == 0 or step==0:\n",
        "        print(f\"Step {step+1}/{steps} | loss_C: {loss_C.item():.4f} \"\n",
        "              f\"| loss_G: {loss_G.item():.4f} \"\n",
        "              f\"| cycle_loss: {cycle_loss.item():.4f} \"\n",
        "              f\"| reward: {reward_vals.mean().item():.4f}\")\n",
        "\n",
        "print(\"✅ Phase-6 training finished: HQ-Cycle-MolGAN (Adversarial + Cycle + Synthetic Reward).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zY3_sEMmgIy",
        "outputId": "c558509b-0513-4fd0-8cb4-062e65dd0b9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1/300 | loss_C: -2.2216 | loss_G: 8.7303 | cycle_loss: 1.1642 | reward: 0.5077\n",
            "Step 50/300 | loss_C: -2.1954 | loss_G: 5.9414 | cycle_loss: 0.8731 | reward: 0.5077\n",
            "Step 100/300 | loss_C: -2.0074 | loss_G: 5.0709 | cycle_loss: 0.8023 | reward: 0.5080\n",
            "Step 150/300 | loss_C: -1.8554 | loss_G: 9.1027 | cycle_loss: 1.2062 | reward: 0.5086\n",
            "Step 200/300 | loss_C: -2.0954 | loss_G: 6.3778 | cycle_loss: 0.9082 | reward: 0.5083\n",
            "Step 250/300 | loss_C: -1.6971 | loss_G: 5.9575 | cycle_loss: 0.8656 | reward: 0.5081\n",
            "Step 300/300 | loss_C: -1.9689 | loss_G: 5.9378 | cycle_loss: 0.8674 | reward: 0.5086\n",
            "✅ Phase-6 training finished: HQ-Cycle-MolGAN (Adversarial + Cycle + Synthetic Reward).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Why We dont get the Diagrams -\n",
        "1.We haven’t implemented the decoder (fake graph vector → RDKit molecule).\n",
        "\n",
        "2.And since RDKit wasn’t installed, we avoided molecule visualization altogether.\n",
        "\n",
        "##🔹 Without RDKit, can we still visualize molecules?\n",
        "\n",
        "Strictly speaking — we can’t render chemical structures (atoms/bonds) without a cheminformatics toolkit like RDKit, OpenBabel, or ChemAxon. These libraries know the rules of valence, atom symbols, bond types, etc.\n",
        "\n",
        "But — since your Generator already outputs adjacency (9×9×5) and features (9×5), you can visualize these as graph diagrams instead of “chemical” diagrams.\n",
        "\n",
        "##🔹 Alternative Visualization Methods (no RDKit)\n",
        "\n",
        "NetworkX Graph Visualization\n",
        "\n",
        "**1.Treat atoms as nodes, bonds as edges.**\n",
        "```\n",
        "Node color = atom type (C, O, N, etc.)\n",
        "\n",
        "Edge style = bond type (single, double, triple).\n",
        "\n",
        "Use networkx.draw + matplotlib.\n",
        "\n",
        "👉 You won’t get perfect chemistry (valence checks, aromaticity, etc.), but you’ll get a graph-like diagram showing how the Generator connects nodes.\n",
        "```\n",
        "\n",
        "**2.Heatmaps**\n",
        "```\n",
        "Plot adjacency tensor slices (each bond type).\n",
        "\n",
        "Each is a 9×9 matrix — heatmap shows how strongly the Generator predicted each edge.\n",
        "\n",
        "Atom features (9×5) can be plotted as a color-coded matrix.\n",
        "```\n",
        "\n",
        "**3.Graph Embedding Visualization**\n",
        "```\n",
        "Project the generated graph vectors into 2D (via PCA or t-SNE).\n",
        "\n",
        "Plot them alongside real QM9 vectors.\n",
        "\n",
        "This shows whether Generator’s outputs “look like” real molecules, even without drawing molecules.\n",
        "```"
      ],
      "metadata": {
        "id": "-5TE666ftZt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here We are Implementing the Visualisation Part without RDkit\n",
        "# We will use  networkx  and matplotlib\n",
        "\n",
        "!pip install networkx matplotlib\n",
        "\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_graph(vec):\n",
        "    # Split vector into adjacency (9x9x5) and features (9x5)\n",
        "    adj = vec[:GRAPH_ADJ_FLAT].reshape(9,9,5)\n",
        "    feats = vec[GRAPH_ADJ_FLAT:].reshape(9,5)\n",
        "\n",
        "    # Pick most likely atom for each node\n",
        "    atom_ids = feats.argmax(axis=1)\n",
        "    atom_labels = [ATOM_TYPES[i] if i < len(ATOM_TYPES) else \"X\" for i in atom_ids]\n",
        "\n",
        "    # Create graph\n",
        "    G = nx.Graph()\n",
        "    for i, label in enumerate(atom_labels):\n",
        "        if feats[i].max() > 0.5:  # keep active nodes\n",
        "            G.add_node(i, label=label)\n",
        "\n",
        "    bond_types = [\"-\", \"=\", \"#\", \"ar\", \"other\"]\n",
        "    for i in range(9):\n",
        "        for j in range(i+1,9):\n",
        "            bt = adj[i,j].argmax()\n",
        "            if adj[i,j].max() > 0.5:  # threshold\n",
        "                G.add_edge(i, j, bond=bond_types[bt])\n",
        "\n",
        "    # Draw\n",
        "    labels = nx.get_node_attributes(G, 'label')\n",
        "    edge_labels = nx.get_edge_attributes(G, 'bond')\n",
        "    pos = nx.spring_layout(G)\n",
        "    nx.draw(G, pos, labels=labels, with_labels=True, node_color=\"lightblue\", node_size=800)\n",
        "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
        "    plt.show()\n",
        "\n",
        "# Example: visualize one fake molecule\n",
        "z = torch.randn(1, noise_dim, device=device).float()\n",
        "fake_vec = G(z).detach().cpu().numpy()[0]\n",
        "visualize_graph(fake_vec)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        },
        "id": "5q-bj9VSvE03",
        "outputId": "8627430d-54f0-4ce9-b6f6-b20148f09d0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (3.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAApQAAAHzCAYAAACe1o1DAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAXYRJREFUeJzt3Xd81eX5//H3OclJQgYZJGFEkFhkKYJQCBKUqbb8nJVCFSFVsa7WXQdgXeDW1om2WA2gddf1tVqZKiAgCDgQBILIyk5IcpKc9fn9ATllBcgZ+Zzxej4efXwxybnPpV8I79yf+7pui2EYhgAAAAAfWc0uAAAAAOGNQAkAAAC/ECgBAADgFwIlAAAA/EKgBAAAgF8IlAAAAPALgRIAAAB+IVACAADALwRKAAAA+IVACQAAAL8QKAEAAOAXAiUAAAD8QqAEAACAXwiUAAAA8AuBEgAAAH4hUAIAAMAvBEoAAAD4hUAJAAAAvxAoAQAA4BcCJQAAAPxCoAQAAIBfCJQAAADwC4ESAAAAfiFQAgAAwC8ESgAAAPiFQAkAAAC/ECgBAADgFwIlAAAA/EKgBAAAgF8IlAAAAPBLrNkFAAAAhDqPYaim0aXKRqeqGpyqanTK5TbkMQxZLRbFxliUFm9TWoJN6fE2pcTHymqxmF12q7EYhmGYXQQAAEAoqnW4VFRlV1G1XS7P3shkkXS48LT/x2OtFuWmJio3LVHJcZG/f0egBAAAOEhFvUPfl9WoxO5oNkAeTdPrshPj1DszRRlt4gJbZAghUAIAAOzj9hhaX16jjRV1PgfJgzWt0z0jSb3apSjGGnmPwgmUAAAAkiobnFqxs1J1TnfQ3iPJFqNBndKVnmAL2nuYgUAJAACiXkldo5buqJBhBGZXsjkWSRaLNCQnQ9lJ8UF8p9ZFoASAAKELFAhPJXWNWrK9IqhB8mAWSfnHRU6oJFACgJ/oAgXCV2WDU4u3lcljQhqyWqRhXTIj4vE3gRIAfEQXKBDe3B5D87aWyu50t+ru5P6SbTEa1TUr7Bt1CJQA0EJ0gQKR4dvSPdpYUWd2GeqekaSTs9qaXYZfCJQA0AJ0gQKRoaLeoUXbys0uw2t4l3Zh/YSCu7wB4BiV1DVq8bYy2YMYJiXJ7nRr8bYyldQ1BvV9gGj2fVmNQuU5gEV76wlnBEoAOAZNXaCeII8Ukfau7zGkJdsrCJVAENQ6XCqxO0w7N3kwQ1KJ3aFah8vsUnxGWyEAHEVlg3PvfLpWfl9D0tIdFRHTBQqEiqIqe0DOP+/etlXvznpOa5d+psqSYsXabOrSvaeG/Po8nTluguIT2hzzWpZ9dfXJDs+zlARKADgCt8fQip2VMuu0uceQVu6sjIguUCAUeAxDRdV2v8PkqkXz9NiNf5AtLl7Dzh+rLif2lMvp0PpVKzTn0fv1848bdM39jx7zeoakomq7TspKCcv5tARKADiC9eU1QW3AORa1TrfWl9eEfRcoEApqGl3eebG+Kt6+TU/cfI2yOh2ne19+U+nZ7b2f+/WEy7TrpyKtWjy/xeu6PIZqHC6lxoffEwnOUAJAMyrqHSExUkSSNlbUqaLeYXYZQNirbHT6vca7s55Tg71O105//IAw2aTj8bk6Z9Jkn9auavC/PjMQKAGgGXSBApGnqsHp95/rrxZ+qvadj1fP/gMDUlMTi/ae2Q5HBEoAOAy6QIHIVNXo9OvPtb22RhXFu9Sle8+A1dTEkFQdgB1UM3CGEgAOIxBdoIHsAJXCvwsUaAmPx6PGxkY5HA7v//b/Z19/3fu8i5WUkeVzXfW1e58UtElKDtS/6gGc7lD5MbZlCJQAcJBAdIEGugNUCv8uUIQOwzDkcrkCFtKC8Wu3279mOIvFovj4eMXHxysuLk5xcXGKj4/X7WdeqCQ/1m2TnCJJqq+r9au+5njC9AJDAiUAHMTfLtBgdYBK4d0FGi0Mw5BhGLJarYd8vLi4WCUlJaaHNofDIX9vXrbZbN6Qtn9ga+7XCQkJatu2bbNf05K1juXXMTExh617XlGp9vhxdCQxOUUZ2R30848bfF7jSML1h0UCJQAcxN8u0GB2gEp7mwqiOVAahiG3221aGOvSpYvmzp17SF2zZs3Sn/70J8XGxqpNmzb69NNP1bdvX+/nPR6Ppk+frmefffao/45Nu2stCVBt27YNekjbf11LuAafGP/rHjB8tD59Y642fP2Vepz6ywBU9T+2ANRnBovh748oABBh1hRXq6jK90feVw4bIFtcnJ77dFlA65L2nqPMTUtUv/apAV+7icfjkdPpDInHns39OlC7a76EqS5dumj69OmHrFlXV6fKykrZbDbFxMQoPT39gF0ywzBUVFSksrKyo75HbCz7PcHi759vae/56FsuGK3MfU8h0jKzDvn8V4vmtfgHx9b48x0s/I4FgIP40wXa1AE6cNTZAa2piSHph60/a/FrLx/zo82Whjan078d2v131441pCUnJ6tdu3ZB3VVr+rXNZjvkcXQgJCUlKSmp+dN5FotFJ5xwgk444YSAvzeOXVqCze/pDR26dNWNjz2rJ266Rjf8v2H7zkn3kMvp1Iavv9LSjz/UiAvHtXhdQwrba1YJlABwEJcfXZbB7gCVpF0lJbpr6tRjeizZ9M/JyclBDWn7/zomJiZsH4ci8qUH6LjIwJFn6/H35um9F2dq5fxP9Mm/ZssWF6fje/RSwe1/0ZnjJvi0bhqBEgAigz9dlsHuAJWkk046WbW1wVsfiGQp8bGKtVr8vn5Rkjp1PaHF0xqOJNZqUUpceEaz8KwaAPywZ88e7dixQzt37tSOHTsO+fXEex9Tp9xuPq0d7A5QKXy7QIFQYLVYlJuaqE2VdSFzcYG07/xkamLY/vkmUAKIGE6nU7t37/YGw+ZC48G7e+np6crJyVFOTo5OOukktT3CObhjEcwOUCl8u0CBUJGblqgfK+vMLuMAhvbWFa7o8gYQ8gzDUGVl5WFD4v7/XFJSckD3b1xcnDcodurU6bC/7tixoxITD/wm7m8XaLA6QKXw7gIFQskXP5erNESuV7VIykqM09DO7cwuxWcESgCmamho8AbC5oLizp071dDQcMDrsrOzDwiGhwuN7dq186k5ZGu1Xat3V/v177VywSd64qZrFJeQ0GwH6NX3PeLT2gM6pOr41PDdyQBCQUW9Q4t+KpNC5BHz8C7tlNEmzuwyfEagBBAUHo9HpaWlzYbEpv9VVFQc8LrExMQDQuLhgmLHjh0VFxe8b7zVDU7N/6nM73V2bt2i916cqXVLP1NFSbG3AzR/zPk6c9wE2eLifVp3VNfMqB5sDvjL4XDoL3/5i3a6Y3X+5dfIEoQxUi3RPSNJJ2e1NbUGfxEoAbRYbW3tEYPizp07tWvXrgPmGVqtVnXo0OGwIXH/f27btq3pI2c8hqEPNxUHpAs00GKtFp3TrX3YHtwHzLZ+/XpNmDBB3377rWY88IBO+c0k2Z1uUx59WyQl2WI0qmuWYqzh/WeaphwAXi6XS8XFxUc9q7hnz54DXpeamuoNhj169NDIkSMPCY3t27dv9m7dUEMXKBB5DMPQc889p1tvvVVdu3bVl19+qf79+6uywanF28pkxvaaxSIN7JQe9mFSYocSiAqGYai6uvqInc87duxQcXGxPB6P93U2m80bCptraunUqdMRbwcJV7UOl/5bVGp2GYc4KzdLyWE6pw4wS3FxsS6//HJ99NFHuu666/TII48c0IxXUteoJdsrWvUHSIuk/OMylJ3k29GXUBN1gdJjGKppdKmy0amqBqeqGp1yuQ15DENWi0WxMRalxduUlmBTerxNKfGx7AYgpDkcDu3cufOoj6DtdvsBr8vMzDxqUMzMzAzKFXXhIpS6QN0ulxrKizV24Elq06aN2eUAYeODDz7QFVdcIavVqn/+858aM2bMYb+upK5RS3dUyDAU9D/zVos0JCdywqQURYGy1uFSUZVdRdV277koiw7/m2b/j8da9z76yk1LZFcArcowDJWVlTW7m9j0z6WlB+6iJSQkHLWppVOnToqPj5xvZMFSUe/Qom3lZpchae/vh6kXn6f6ilI99NBD+t3vfmf6WVMglNXV1emWW27RCy+8oHPPPVezZs1Sdnb2EV9T2eDUip2VqnO6g1ZXsi1GAzulh+2d3c2J+EBZUe/Q92U1KrE7mg2QR9P0uuzEOPXOTAnrtn6EBrvdftSguHPnTjkcDu9rLBaL2rdvf9SmlrS0NIJGAH1bukcbK8wfgNw9I0nxVcW6/fbb9e9//1t5eXl64oknNGTIELNLA0LOV199pQkTJmj79u3661//qiuvvPKYvy+6PYbWl9doY0Wdz7nhYE3rdM9IUq92KRFxZvJgERso+Q0BMxiGccBNLc2FxqqqqgNel5KSctQB3B06dFBsLLvkrc3tMTRva2lIdYEuWrRIN998s77++muNGzdODz30kHJzc02oDggtbrdbDz30kO655x7169dPr7zyirp37+7TWmxItUxEBsrW2LJOssVoUARuWePIDMNQY2OjampqZLfblZOTc0DIKy8vV2ZmpvefY2Nj1bFjx6OeVUxJSTHjXwfHqKkL1IwpQlaLNKxL5iHfazwej+bMmaMpU6aorKxMN954o6ZMmaLUVG7QQXQqKirSxIkTtWzZMt155526++67ZbP5/3c0R+aOTcQFytY6VGvR3nb/SDtUG42cTqd27dp1wA7i8OHDdfLJJx/QkOJ2u/WXv/xFf/vb35Sdna28vDw9//zzSktL836NYRj6v//7P+85xezs7KhuaokkodoFWldXp8cee8zbtXrffffpyiuvZDcbUcMwDM2dO1fXXXed2rVrpzlz5mjo0KEBfx+PYajG4VJVg1OVDU5VNzrl3K+p1xZjUWq8TekJext7U+Kiq6k3ogJlqH7DhzkOvv+5ubOKB9//HB8fr5kzZ+rSSy895Kdbu90uwzAickwOji6Uu0B37NihadOmqbCwUD179tTjjz+uX/3qV5ynRUSrrKzU1VdfrTfeeEOTJk3SU089xS69SSImUIbiIykEz/73Pzd3VrG5+5+P1tSSkZHBX8JoVqh3ga5evVq33HKLFi1apLPOOkuPPfaY+vTpE4QqAXMtWLBABQUFqq2t1QsvvKBx48aZXVJUi4hAafaheWnvXwCRcHWS2Zrufz7SLS2Hu/85KSnpmJpagnn/M6JHqDf9GYah999/X3/+85+1efNmXXHFFbrvvvvUoUOHAFQKmKuxsVHTpk3T448/rhEjRqiwsFDHHXec2WVFvYgIlKE01iPcL3cPptra2iN2Pu/YsUO7du2Sy+XyviYmJsZ7/3NzQTEnJ0cpKSnsKqLVhXoXqMPh0PPPP6977rlHTqdTU6ZM0Y033shgdISt7777ThMmTND69ev1wAMP6KabbuKceogI+0AZSoOHJWl4l3YRPRbgcA6+/7m50Hjw/c9paWlH7X4Op/ufEb1CvQu0oqJC06dP19NPP61OnToxGB1hxzAMPfPMM7rtttt0wgkn6NVXX1Xfvn3NLgv7CftAGUpXo1kkZSXGaWjndmaXEhAH3//cXFA80v3PR7qphcYWRJpQ7wL98ccfGYyOsLNr1y5ddtll+uSTT3T99dfroYceYpc9BIV1oKx1uPTfotKjf2ErOys3K+RnTjXd/3yks4pHuv/5SE0t7dq14xEEEMIYjI5w8e6772ry5Mmy2Wx6+eWXdfbZZ5tdEpoR1oHym5I92lRZ59fu5O5tW/XurOe0dulnqiwpVqzNpi7de2rIr8/TmeMmKD6hZT8FWSR1S09Sn2xzzlI23f98tLOKZWVlB7yuTZs2Rw2KHTt25P5nIEIwGB2hrLa2VjfddJNmzZqlCy64QP/4xz8OuDQCoSdsA6XHMPThpmLveSVfrFo0T4/d+AfZ4uI17Pyx6nJiT7mcDq1ftULLP/1Iwy8Yp2vuf7TF68ZaLTqnW/uAP8o6+P7nw4XGg+9/tlqt3vufj9TUkpqaynkqIAoxGB2hZvny5br00ku1a9cuPfnkk7r88sv5+ykMhG2grG5wav5PZUf/wmYUb9+mm88bpXYdOurel99Uenb7Az6/66cirVo8X+dMmuzT+qO6Zio1/thmyLndbpWUlBxx+Pbh7n9u27btUYNi+/bt+YsBwFExGB1mc7lceuCBB3TfffdpwIABmjt3rk488USzy8IxCttAubXartW7q31+/Qv33KH/vjZbM159Tz37DwxgZXsN6JCq41MTtWfPnqMGxd27d8vt/t+Q5NjYWG/jypFmKyYnJwe8bgDRjcHoMMOWLVt06aWXavny5Zo2bZqmTZsWkHu40XrCNlCuKa5WUZXd5/OTVw4bIFtcnJ77dFlA65Ikt8ul5R+/rxfuuUO1tbUHfC4jI+OoZxWzsrJoagFgmoMHo0+ePFn33Xef2rdvf/QXAy1gGIYKCwv1pz/9SdnZ2Zo7d65OO+00s8uCD8I2UC7aVqaKeqdPr7XX1mjiL3to4KizdcezLwW4sr327NquitWfHxAUO3XqpISEhKC8HwAEmsPh0MyZM3XvvfcyGB0BV15erquvvlpvvfWWLrvsMj355JNKSUkxuyz4KGwD5byiUu1xuI7+hYdRvnun/jD8lzrjvIt0wyNPB7iyvdrGxWp0blZQ1gaA1sRgdATavHnzVFBQoPr6ev3973/X2LFjzS4Jfgrb56oeP3Jwm+S9PwHV19Ue5St95099ABBKMjIy9MQTT+j777/XgAEDdMkll+i0007T0qVLzS4NYejdd9/VmWeeqd69e+ubb74hTEaIsA2U/ozkSUxOUUZ2B/3844YAVnSg1rz9AgBaw4knnqh33nlHCxculMPhUH5+vsaPH6+ioiKzS0MYGTNmjAoLC/XJJ58oJyfH7HIQIGEbKGNj/AtsA4aP1u5tW7Xh668CVNGBbH7WBwChavjw4frqq6/08ssv64svvlDPnj11++23q7ra98kbiB42m02TJk2i+TTChO3/N9PibfInsl0w+VolJCbqubtuVVXZodc37t62VR/OnuXT2hbpmGdQAkA4slqtKigo0MaNGzVlyhQ988wz6tatm2bOnCmXy7fz7YgOnL2NTGHblOPvHEpJWrngEz1x0zWKS0jYd1NOD7mcTm34+ist/fhDjbhwnK6+7xGf1m6aQwkA0WD/weiffvqpRowYwQ5UlHO73Xrqqad0wQUXcFd8FAjbQOnvTTlNdm7dovdenKl1Sz9TRUmxbHFxOr5HL+WPOV9njpsgW5xvd1e35KYcAIgUGzduVPfu3c0uAybbunWrRo0apb59+2r27NlcxBEFwjZQBuIu72AJ1l3eABAODMM47GPN+vp6JSQk8MgzwlVVVenyyy/XmDFjNHmyb9cXI/yE7fMIq8Wi3NREv85RBoNFUm5qImESQNRqLjAuXrxYXbt21aJFi1q3ILSqtLQ0zZ07lzAZZcJ2h1KSah0u/bfo0IYas52Vm6XkuFizywCAkPPRRx/pjjvuUE5Ojt59913Fx/t2rAiho6GhQbGxsYqN5e+9aBa2O5SSlBwXq+zEuJDZpbRIyk6MI0wCQDPGjBmjVatWacaMGYqJiTG7HPjBMAy9+OKL+uUvfymn06kw3p9CAIR1oJSk3pkpCpXfwob21gMAaJ7NZlP//v29O1oEkfBTVlamiy66SJMnT9bgwYNlsVg4Gxvlwj5QZrSJU/eMJLPLkCR1z0hSRps4s8sAgLDidrv16KOPMhg9TPz3v//VKaecos8++0zvvPOOZs2apYSEBLPLgsnCPlBKUq92KUqyxZj26NsiKdkWo17t2J0EgJYwDEOrVq3SPffcw2D0EFdfX68bbrhBZ599tvr06aN169bpwgsvNLsshIiICJQxVosGdUqXWbvtFos0sFO6Yqxs9wNAS1gsFuXl5Wnjxo0655xzdN111+mUU07Rf/7zHx6Fh5C1a9dq4MCBeuGFF/Tkk0/qP//5jzp16mR2WQghEREoJSk9waYhORmtvktpkTQkJ0PpCQwxBwBf5eTk6KWXXtJXX32l9u3ba8yYMfrVr36lb775xuzSoprH49Hjjz+uQYMGKSYmRqtWrdL111/PLUg4RET9jshOilf+cRmyWtQqwdJqkfKPy1B2EmMvACAQ+vfvrwULFujdd99VUVGR+vXrp6uuukrFxcVmlxZ1tm/frjPPPFO33nqrrr/+eq1YsUInnXSS2WUhRIX1HMrmVDY4tWJnpeqc7qC9R7ItRgM7pbMzCQBB4nA4NHPmTN17771yOp2aMmWKbrzxRrVp08bs0iLem2++qauuukqJiYmaPXu2Ro4caXZJCHERGSglye0xtL68Rhsr6mSRAjJaqGmd7hlJ6tUuhTOTANAKKioqdP/99+uZZ55Rp06d9NBDD+l3v/sdY2qCYM+ePfrTn/6k2bNna9y4cZo5c6YyMjLMLgthIGIDZZOKeoe+L6tRid3hc7Bsel12Ypx6Z6YwGggATPDjjz/q9ttv17///W/l5eXpiSee0JAhQ8wuK2IsWbJEl156qcrLy/XMM89o4sSJhHYcs4g6Q3k4GW3iNLRzO52Vm6Vu6UmK3W9Xsbk/Jm63y9tdGGu1qFt6ks7KzdLQzu0IkwBgkhNPPFHvvPOOFi5cKIfDofz8fI0fP15FRUVmlxbWnE6n7rrrLp1xxhnKycnR2rVrNWnSJMIkWiTidygP5jEM1ThcqmpwqrLBqepGp5xuQx7DkNVikS3Goi3ff6vXXnpRr/7z70pPTJCVP1QAEFI8Ho/mzJmjKVOmqKysTDfeeKOmTJmi1NRUs0sLKxs3btSll16qr7/+Wvfcc49uv/127uSGT6IuUB6LZcuWaciQIVq+fLkGDRpkdjkAgGbU1dXpscce0yOPPKLExETdd999uvLKKwlFR2EYhv7xj3/opptuUk5Ojl555RUNHDjQ7LIQxiL+kbcv+vfvr/j4eC1ZssTsUgAAR5CUlKS7776bwegtUFpaqgsuuEBXXXWVd3eSMAl/ESgPIz4+XoMGDSJQAkCYYDD6sfnPf/6jPn36aOnSpXrvvff0wgsvKCkpyeyyEAEIlM3Iz8/XkiVL+AkXAMIIg9EPr76+Xn/60580ZswY9e/fX998843OO+88s8tCBCFQNiM/P1+7d++mexAAwozFYtH555+vb7/9Vk888YTefPNNdevWTQ8++KDq6+vNLq/VrVmzRgMGDNCsWbP0zDPP6P/+7//UoUMHs8tChKEppxkVFRVq166dZs+erYkTJ5pdDgDAR2YORvcYhmoaXapsdKqqwamqRqdc+00WiY2xKC3eprQEm9LjbUqJjw3YZJGme7inTp2qk046Sa+88op69+4dkLWBgxEoj6B37946/fTT9cILL5hdCgDAT605GL3W4VJRlV1F1Xa5PHv/mm3uco39Px5rtSg3NVG5aYlKjvO9U/3nn3/WpEmTtHjxYv35z3/W/fffr7g45igjeHjkfQRN5ygBAOGvNQajV9Q79MXP5fpvUak2VdZ5w6TU/E1t+3/c5TG0qbJO/y0q1Rc/l6ui3tHiGl577TWdcsop2rx5sxYsWKCHH36YMImgI1AeQX5+vr777jtVVlaaXQoAIECGDx+ur776Si+//LK++OIL9ezZU7fffruqq6t9XtPtMfRt6R4t2lauUvveEOjr47+m15XaHVq0rVzflu6R23P01aqrqzVx4kRdfPHF+tWvfqW1a9dq+PDhPlYBtAyB8giGDh0qae+gcwBA5LBarSooKNDGjRs1ZcoUPfPMM+rWrZtmzpwpl8vVorUqG5yat7VUGyvqJPkeJA/WtM7GijrN21qqygZns1/7+eefq2/fvnr//fc1d+5c/etf/1J6enqAKgGOjkB5BL/4xS+UnZ3NY28AiFD+DkYvqWvU4m1lsjvdQa3T7nRr8bYyldQ1HvBxh8OhqVOnavjw4erSpYvWrVunCRMmBLUW4HAIlEdgsVg4RwkAUcCXwegldY1asr1CHiNwu5LNMSR5DGnJ9gpvqNywYYOGDBmiRx55RDNmzNDChQt1/PHHB7kS4PAIlEeRn5+v5cuXy+Fo+cFoAEB4OdbB6JUNTi3dURH0IHkwQ9LSHRWaNfdfOvXUU1VbW6svv/xSd9xxh2JiYlq5GuB/CJRHkZ+fr4aGBn399ddmlwIAaAVHG4zu9hhasbNSZg3dc7ndasjsrMuvuEKrV6/WgAEDzCkE2A9zKI/C4XAoNTVVM2bM0M0332x2OQCAVnbwYPRHZ7+h2A5dzC3KMNS9XbJOzmprbh3APuxQHkVcXJwGDRrEOUoAiFIZGRn661//qu+++05nnf8bWbOPM7skyWLRxoo6n+ZUAsFAoDwGI0eOVGxs7DF1/AEAIlP37t1V8OdpAbsa0V8WSd+X1ZhdBiCJR97HxDAMWSwW7/8FAESfWodL/y0qNbuMQ5yVm+XXNY1AIPA78Bg0hUjCJABEr6Iqe7P3cR+rbT9u0L///rS+Xb5UeyorlJKWrpPzhug3V12vLif2aPF6ln119cnmLCXMxQ4lAABH4TEMfbip+IC7uVvqy/9+pL/ecq2S09I06qKLlX1cZ5Xu2K75b/1LNVWVuvmJmco789ctXjfWatE53dqHzKN4RCcCJQAAR1Hd4NT8n8p8fv3ubVt18/mjlNkxR/fP/bdSM9p5P7enslzTJlyo8t079fh789Whc8uHk4/qmqnUeJvP9QH+oikHAICjqGxs/h7tY/HeizPVWF+vq+979IAwKUlt09vpqnsfUYPdrvdmPefT+lVHuOcbaA0ESgAAjqKqwSl/Hih/tfBTZed0Vu9f5h328ycNHKzsnM5atXhei9e2aO/NPYCZCJQ+MAxDZWW+P/oAAISXqkanz804dTV7VFGyW8f37H3Erzu+Ry+V796l+traFq1vSKr2cwcV8BeB0gdOp1OTJ09WQ0OD2aUAAFqBy+17u0FD3d6A2CYp+Yhf1/R5e13LZ0s6/agPCAQCpQ/i4uJUUlKi7777zuxSAACtwONH/2rCvqBYX3fkncf6Ywyeh+NPfUAgECh9NHToUL333ntmlwEAaAX+jORJSmmr9Kz2+mnD+iN+3U8b1iujfUclJqe0+D0YGQSzESh9NHHiRHXt2tXsMgAArSA2xr/ANmD4aJVs36b1q5Yf9vPff7VcJTt+1i+Hj/ZpfZuf9QH+Yg6lH7iKEQCiw5riahVV2X1uzNm5dYtuuWC0so/roulz3lFKeob3czVVlZp26YUq3fGznnhvvjp06dqitS2SctMS1a99qo/VAf4jUAIAcBRbq+1avbvarzWWfvyBnvzzH5WSlqFRY/felFOy42cteOs17ams0E2PP6fBZ43xae0BHVJ1fGqiX/UB/iBQ+sDlcsnlcikhIcHsUgAArcDfm3Ka/LRhvd75+9P6bsUy1VRVKDktXScPGqKLrrpeXbr39HldbsqB2QiULVRTU6P27dtr5syZKigoMLscAEArCMRd3sHCXd4IBTTltFBKSopOOOEEffHFF2aXAgBoJVaLRbmpiX7dlhMMFkm5qYmESZiOQOmD/Px8LVmyxOwyAACtKDct0eemnGAxtLcuwGwESh/k5+dr/fr1qqioMLsUAEArSY6LVXZiXMjsUlokZSfGKTku1uxSAAKlL4YOHSpJWrp0qcmVAABaU+/MlJDZpTS0tx4gFBAofZCbm6sOHTrw2BsAokxGmzilNNbI4/GYXYq6ZyQpo02c2WUAkgiUPrFYLJyjBIAoYxiGnn32WV0wZICqSnZLJu1VWiQl22LUqx27kwgdBEof5efna8WKFWpsbDS7FABAkNXU1OiSSy7RH//4R11x+WU6r38v0zqrLRZpYKd0xVhD5TQnQKD0WX5+vhobG7V69WqzSwEABNF3332ngQMH6sMPP9Qbb7yhp556StkpiRqSk9HqDToWSUNyMpSewBBzhBYCpY9OPfVUtWnThsfeABDB5syZo0GDBslms+mrr77Sb3/7W+/nspPilX9chqwWtUqwtFqk/OMylJ0U3wrvBrQMgdJHNptNgwYNIlACQARqaGjQH/7wB02aNEm//e1vtXz5cvXo0eOQr8tOitewLplKtMUEtZ5kW4yGdckkTCJkESj9MHToUC1ZskTcXgkAkWPz5s0aMmSI5syZo1mzZumll15SYmLzw8PTE2wa3TVL3TOSJAVut7Jpne4ZSRrVNYvH3AhpBEo/5Ofnq7S0VJs2bTK7FABAALz77rsaMGCA9uzZo2XLlumKK66Q5Riab2KsFp2c1VbDu7RTVuLeUT6+Bsum12Ulxml4l3Y6OastDTgIeQRKP5x22mmyWCw89gaAMOd0OnXrrbfqwgsv1KhRo7Rq1Sr169evxetktInT0M7tdFZulrqlJyl2vyDYXCTc/+OxVou6pSfprNwsDe3cjjmTCBsWg+e1funTp4/y8vI0a9Yss0sBAPhgx44dGj9+vJYvX65HHnlEN9544zHtSh4Lj2GoxuFSVYNTlQ1OVTc65XQb8hiGrBaLbDEWpcbblJ5gU1qCTSlxsaaNIwL8wQWgfsrPz9fixYvNLgMA4IN58+bpkksuUXx8vBYvXqwhQ4YEdH2rZW9gTI236fjUgC4NhBQeefspPz9fP/zwg8rLy80uBQBwjDwej+677z6dddZZOvXUU7V69eqAh0kgmhAo/TR06FBJ0tKlS02uBABwLMrKyjRmzBjdc889uvvuu/XRRx8pKyvL7LKAsMYjbz91Of54Dcg/XRuKK9W5uFpVjU659jsfExtjUVr83rMx6fE2pcRzPgYAzLJs2TKNGzdODQ0N+uSTT3TmmWeaXRIQEWjK8VGtw6WiKruKqu1yeQwZ+wLk4f5jWiTvx2OtFuWmJio3LVHJceR5AGgNhmHoySef1J///GcNGjRIr7/+uo477jizywIiBoGyhSrqHfq+rEYldscBQbElml6XnRin3pkpjIUAgCCqrq7WFVdcobffflu33HKLHnzwQdlsDAkHAolAeYzcHkPry2u0saLO5yB5sKZ1umckqVe7FAbXAkCArV27VmPHjlVJSYleeukl/eY3vzG7JCAi0ZRzDCobnJq3tVQbK+okBSZM7r/Oxoo6zdtaqsoGZ4BWBgD885//1ODBg5WcnKxVq1YRJoEgIlAeRUldoxZvK5Pd6Q7q+9idbi3eVqaSusagvg8ARDq73a7LLrtMV1xxhSZOnKilS5eqW7duZpcFRDQeeR9BSV2jlmyvCNiO5LGwSMo/LkPZSfGt+K4AEBk2btyosWPHatOmTXr++ec1adIks0sCogI7lM2obHBq6Y7WDZPS3sfgS3dU8PgbAFrozTff1C9/+Us5HA4tX76cMAm0IgLlYbg9hlbsrJRZe7ceQ1q5s1JuD5vHAHA0DodDN9xwg8aNG6cxY8Zo5cqV6tOnj9llAVGFQHkY68trVOd0t/ru5P5qnW6tL68xsQIACH3btm3TGWecoZkzZ+qZZ57Rv/71L6WkpJhdFhB1mKx9kIp6h7eb22wbK+rUKTmBOZUAcBgff/yxJkyYoOTkZH3xxRcaNGiQ2SUBUYsdyoN8X1ajUJkGadHeegAA/+N2u3XXXXdpzJgxGjx4sFavXk2YBEzGDuV+ah0uldgdZpfhZUgqsTtU63BxTSMASCouLtaECRO0cOFCzZgxQ7fffrusVvZGALORUvZTVGX3+xac3du26t1Zz2nt0s9UWVKsWJtNXbr31JBfn6czx01QfEKbFq1n2VdXn+y2flQFAOHv888/1/jx4+XxeDRv3jyNGDHC7JIA7EOg3MdjGCqqtvsVJlctmqfHbvyDbHHxGnb+WHU5sadcTofWr1qhOY/er59/3KBr7n+0RWsakoqq7TopK0VWS6g8jAeA1mMYhh577DHdeeedys/P12uvvaaOHTuaXRaA/RAo96lpdMnlx5ie4u3b9MTN1yir03G69+U3lZ7d3vu5X0+4TLt+KtKqxfN9WtvlMVTjcCk13uZzfQAQjiorK/X73/9e77//vm6//XZNnz5dsbH81QWEGv5U7lPZ6N8g8XdnPacGe52unf74AWGyScfjc3XOpMk+r1/V4CRQAogqq1at0m9/+1tVVlbqgw8+0DnnnGN2SQCawUnmfaoanH51d3+18FO173y8evYfGLCamlgkbs4BEDUMw9ALL7ygIUOGKCMjQ6tXryZMAiGOQLlPVaPT5/OT9toaVRTvUpfuPQNaUxNDUrWfO6gAEA7q6uo0adIkXX311Zo8ebKWLFmi3Nxcs8sCcBQ88t7H5fb9/GR97d5ZkW2SkgNVziGcftQHAOFg/fr1Gjt2rH766Se9+uqruvjii80uCcAxYodyH48fF3e3Sd57zVd9XW2gyjmEP/UBQKh79dVXNXDg3iNDK1euJEwCYYZAuY8/I3kSk1OUkd1BP/+4IYAVHYiRQQAiUWNjo6699lpNmDBBF1xwgVasWKFevXqZXRaAFiJQ7hMb419gGzB8tHZv26oNX38VoIoOZPOzPgAINUVFRRo6dKhefPFFPf/885ozZ46SkpLMLguADwiU+6TF2/zq8r5g8rVKSEzUc3fdqqqy0kM+v3vbVn04e5ZPa1skRgYBiCgffPCB+vfvr/Lyci1btkxXXXWVLDyJAcKWxTA4nCdJW6vtWr272q81Vi74RE/cdI3iEhL23ZTTQy6nUxu+/kpLP/5QIy4cp6vve6TF6xqGoa/ee13dMttq1KhR6tq1q191AoBZXC6Xpk2bpocffljnnXeeXn75ZaWnp5tdFgA/ESj3qW5wav5PZX6vs3PrFr334kytW/qZKkqKZYuL0/E9eil/zPk6c9wE2eLifVr3xSk36ON335bH49EvfvELjRo1SqNGjdLIkSOVmZnpd90AEGwul0vnnHOO5s2bpwcffFC33noru5JAhCBQ7uMxDH24qdiv6xeDJdZq0Tnd2mtPdbUWLVqk+fPna/78+Vq/fr0kqW/fvho9erRGjRql008/XcnJwRtfBAC+crvd+vDDD5WRkaHTTz/d7HIABBCBcj/flOzRpso6nwecB4NFUrf0JPXJbnvI53bu3OkNl/Pnz9f27dsVGxurwYMHewNmXl6ebDbOXwIAgOAhUO6n1uHSf4sObagx21m5WUqOO/IMesMw9OOPP2revHmaP3++Fi5cqMrKSiUlJemMM87QqFGjNHr0aPXp00dWK71YAAAgcAiUB/ni53KV2h0hsUtpkZSVGKehndu1+LVut1tr1qzxBswvvvhC9fX1yszM1MiRI70B84QTTgh84QCiWm1tre69917dcccdateu5d+/AIQfAuVBKuodWrSt3OwyvIZ3aaeMNnF+r9PY2Khly5Z5A+bKlSvldrvVtWvXAxp82rdvH4CqAUSrNWvWaPTo0Zo8ebIefPBBmm6AKEGgPIxvS/doY0Wd2WWoe0aSTs469OxkIFRXV+uzzz7zBszvvvtOktSnTx9vwBw2bJhSUlKC8v4AIs/u3bv1+9//XldffbUuuOACs8sB0IoIlIfh9hiat7VUdqfblEffFklJthiN6pqlGGvr/HS/e/duLViwQPPnz9e8efO0bds2xcTEKC8vzxswBw8erPh438YeAYh8hmHIbrdz2w0QhQiUzahscGrxtjKZMUXIapGGdclUeoI53dmGYWjz5s3e7vEFCxaovLxcbdq08Tb4jBo1Sv369aPBBwAAECiPpKSuUUu2V7TqLqVFUv5xGcpOCp2dQI/Ho7Vr13oD5meffSa73a6MjAxvg8+oUaPUrVs3zksBUaS+vl4rV65Ufn6+YmJizC4HgIkIlEdRUteopTsqZBgKerC0WqQhOaEVJg/H4XDoyy+/9AbM5cuXy+VyqXPnzt75lyNHjlTHjh3NLhVAkGzevFljx45VUVGRtmzZooyMDLNLAmAiAuUxqGxwasXOStU53UF7j2RbjAZ2SjftMbc/ampq9Nlnn3kD5rp16yRJvXv39o4nGjZsmFJTU02uFEAgvPPOO7rsssuUnZ2tt956S3379jW7JAAmI1AeI7fH0PryGm2sqJNFgdmtbFqne0aSerVLabUGnGArKSnxNvjMnz9fRUVFslqtGjhwoDdgnnbaaUpISDC7VAAt4HQ6dccdd+iJJ57QRRddpBdffJEfFAFIIlC2WEW9Q9+X1ajE7vA5WDa9LjsxTr0zUwIyZzKUbdmy5YAGn9LSUiUkJGjo0KHegHnqqadyBgsIYdu3b9f48eO1YsUKPfbYY7r++us5Mw3Ai0Dpo1qHS0VVdhVV2+Xa1wreXMDc/+OxVotyUxOVm5Z41OsUI5HH49G3337rnX+5ePFi1dXVKS0tTSNGjPAGzO7du/OXFRAiPv30U11yySVKSEjQG2+8odNOO83skgCEGAKlnzyGoRqHS1UNTlU2OFXd6JTTbchjGLJaLLLFWJQab1N6gk1pCTalxMXKSlDycjqdWrFihXf+5Zdffimn06mcnBxv9/ioUaOUk5NjdqlA1HG73Zo+fbruvfdenXXWWZo7d64yMzPNLgtACCJQIqTU1dXp888/9wbMNWvWSJJ69uzpDZfDhw9Xenq6uYUCEa60tFQTJkzQvHnzdO+992rq1KnMnQXQLAIlQlpZWZkWLlzoDZibN2+W1WpV//79vSOK8vPz1aZNG7NLBSLG0qVLNW7cODkcDr366qsaPXq02SUBCHEESoSVn376ydvgM3/+fBUXFys+Pl5DhgzxBswBAwYoNjb6zqcC/jIMQ3/729902223KS8vT6+//jrHTQAcEwIlwpZhGPruu++84XLRokWqqalR27ZtNXz4cG/A7NWrFw0+wFFUV1fr8ssv1zvvvKNbb71VDzzwgGy28JuLC8AcBEpEDJfLpZUrV3oD5tKlS+VwONSxY0eNHDnSGzA7d+5sdqlASFmzZo3Gjh2rsrIyvfzyy7rgggvMLglAmCFQImLZ7XZ98cUX3oC5evVqGYahE0880dvgM2LECLVr187sUgFTGIahf/7zn7ruuuvUu3dvvfnmm/rFL35hdlkAwhCBElGjoqLC2+Azf/58bdy4URaLRaeeeqo3YJ5++ulKTEw0u1Qg6Ox2u6699loVFhbqD3/4g5588klurwLgMwIlotbPP/+sBQsWeIes79q1S3FxcTrttNO8AXPgwIGcI0PE2bBhg8aOHastW7bo+eef18SJE80uCUCYI1AC2vvo74cffvCOJ1q0aJGqq6uVkpKiYcOGeQPmySefTIMPwtobb7yhK664Qjk5OXr77bd10kknmV0SgAhAoAQOw+VyafXq1d6AuWTJEjU2Nqq8vFwZGRlmlwe0mMPh0K233qqnn35av/vd7/T3v/9dKSkpZpcFIEIQKIFjUF9fr7Vr12rw4MGHfM7pdOq9995Tx44d1b9/f4asI+T89NNPGjdunL7++mv99a9/1bXXXstOO4CA4h4t4Bi0adPmsGFSkiwWixobG3XnnXcqPT1d119/vVwuVytXCBzef/7zH/Xv31/FxcVasmSJrrvuOsIkgIBjhxIIoOrqatXW1h5wu4hhGNq0aZO6du1Kgw9ajdvt1t13360ZM2bo//2//6fZs2dzXANA0BAogVbwi1/8QsXFxTrjjDM0atQojR49Wn369JHVykMCBN7u3bt1ySWXaPHixZoxY4Zuu+02fq8BCCoCJRBkhmFo1apV3vmXn3/+uRoaGpSZmamRI0d6A+YJJ5xgdqmIAJ999pnGjx8vwzD02muvafjw4WaXBCAKECiBVtbQ0KBly5Z5A+bKlSvldrvVtWtX73iikSNHqn379maXijDi8Xj02GOPacqUKRo6dKj+9a9/qWPHjmaXBSBKECgBk1VXV2vx4sXegPndd99Jkvr06eMNmMOGDWPEC5pVWVmpgoICffDBB7rzzjt13333KTY21uyyAEQRAiUQYnbv3n3ADT7btm1TTEyM8vLyvAFz8ODBio+PN7tUhIBVq1Zp7Nixqq6u1uzZs3XOOeeYXRKAKESgBEKYYRjavHmzd8D6woULVV5erjZt2ngbfEaNGqV+/frRdBFlDMPQ888/rxtvvFGnnHKK3nzzTXXt2tXssgBEKQIlEEY8Ho/Wrl3rDZiff/657Ha7MjIyNGLECI0ePVqjRo1St27dmDUYwWpra3XVVVfp1Vdf1XXXXafHH3+cHWsApiJQAmHM4XDoyy+/9AbM5cuXy+12q3Pnzt7u8ZEjR9KcEUG+//57jR07Vtu2bdOsWbP0u9/9zuySAIBACUSSmpoaffbZZ94Gn3Xr1kmSevfu7Q2Yw4YNU2pqqsmVwhevvPKK/vCHPyg3N1dvvfWWevbsaXZJACCJQAlEtJKSEi1YsMC7g7l161ZZrVYNHDjQGzBPO+00JSQkmF0qjqChoUE33XSTnn/+eU2cOFEzZ85UUlKS2WUBgBeBEogiW7Zs8e5eLliwQKWlpUpISNDQoUO9DT79+/dXTEyM2aVin6KiIo0dO1bfffednn76aU2ePJnzsQBCDoESiFIej0fffPONN2AuXrxYdXV1SktL04gRI7wBs0ePHlEfYDyGoZpGlyobnapqcKqq0SmX25DHMGS1WBQbY1FavE1pCTalx9uUEh8rawD+m73//vsqKChQenq63nrrLfXv3z8A/zYAEHgESgCSJKfTqRUrVnjnX3755ZdyOp3KycnxhstRo0YpJyfH7FJbTa3DpaIqu4qq7XJ59n6rtEg63DfN/T8ea7UoNzVRuWmJSo5r+YBxl8ulqVOn6pFHHtEFF1ygl156SWlpaT7+WwBA8BEoARxWXV2dPv/8c2/AXLNmjSSpZ8+e3nA5fPhwpaenm1toEFTUO/R9WY1K7I5mA+TRNL0uOzFOvTNTlNEm7phet3PnTv3ud7/T0qVL9fDDD+vmm2+O+h1iAKGPQAngmJSVlWnhwoXegLl582ZZrVb179/fO/8yPz9fbdq0MbtUn7k9htaX12hjRZ3PQfJgTet0z0hSr3YpirE2Hw4XLFigiy++WLGxsXr99dc1dOjQAFQAAMFHoATgk59++snbPb5gwQIVFxcrPj5eQ4YM8QbMAQMGhM2d0pUNTq3YWak6pzto75Fki9GgTulKT7Ad8HGPx6MHHnhAd999t0aMGKFXX31V2dnZQasDAAKNQAnAb4Zh6LvvvvM2+CxatEg1NTVq27athg8f7h1R1KtXr5B8fFtS16ilOypkGIHZlWyORZLFIg3JyVB20t6bbcrLyzVx4kR9/PHHmjZtmu6++2667AGEHQIlgIBzuVxauXKlN2AuXbpUDodDHTt21MiRI71nMLt06WJ2qSqpa9SS7RVBDZIHs0jKPy5DRd+u0W9/+1vZ7Xa98sorOvvss1uxCgAIHAIlgKCz2+364osvvAFz9erVMgxDJ554ojdcjhgxQu3atWvVuiobnFq8rUweE74LGm63plxynjLaxOuNN95Q586dW78IAAgQAiWAVldRUaGFCxd6A+bGjRtlsVh06qmnegPm6aefrsTExKDV4PYYmre1VHanu1V3J73v73KpsXaPLurbTW0S4k2oAAACh0AJwHQ///yzN1zOnz9fu3btks1m05AhQ7wBc+DAgbLZbEdf7Bh9W7pHGyvqAraer7pnJOnkrLZmlwEAfiFQAggphmFo/fr1BzT4VFdXKyUlRcOGDfMGzJNPPtnnBp+KeocWbSsPcOW+G96l3THPqQSAUESgBBDSXC6XVq1a5Q2YS5YsUWNjo7Kzsw+4wadr167HvOYXP5er1O4w5VH3wSySshLjNLRz654fBYBAIlACCCv19fVaunSpd8D6qlWr5PF4dMIJJ3jnX44YMUJZWVmHfX2tw6X/FpW2ctVHd1Zulk/XNAJAKCBQAghrVVVVWrRokTdg/vDDD5Kkvn37eudfnn766UpOTpYkfVOyR5sq6/zandy9bavenfWc1i79TJUlxYq12dSle08N+fV5OnPcBMUntOy2IIukbulJ6pPNWUoA4YlACSCi7NixQwsWLPAGzB07dig2NlaDBw/WqNGj1XfcZBlWq8/rr1o0T4/d+AfZ4uI17Pyx6nJiT7mcDq1ftULLP/1Iwy8Yp2vuf7TF68ZaLTqnW3tZQ3DwOwAcDYESQMQyDEMbN27833iibdt179x3fV6vePs23XzeKLXr0FH3vvym0rPbH/D5XT8VadXi+Tpn0mSf1h/VNVOp8YHrZAeA1kKgBBA1tlTWak3xnr33H/rghXvu0H9fm60Zr76nnv0HBrg6aUCHVB2fGrzZmwAQLL4/9wGAMLPH4fbrLvGvFn6q9p2PD0qYtGjvzT0AEI4IlACiRlWj0+dmHHttjSqKd6lL954BramJIam6kUAJIDwRKAFEDZfb9xM+9bU1kqQ2ScmBKucQTj/qAwAzESgBRA2PH0fG2ySnSJLq62oDVc4h/KkPAMxEoAQQNfwZyZOYnKKM7A76+ccNAazoQIwMAhCuCJQAokZsjH+BbcDw0dq9bas2fP1VgCo6kM3P+gDALARKAFEjLd4mfyLbBZOvVUJiop6761ZVlR16fePubVv14exZPq1tkZhBCSBscXEsgKiRlmDz68rFDl266sbHntUTN12jG/7fsH035fSQy+nUhq+/0tKPP9SIC8f5tLYhKT2BQAkgPDHYHEDUqG5wav5PZX6vs3PrFr334kytW/qZKkqKZYuL0/E9eil/zPk6c9wE2eLifVqXm3IAhCsCJYCo4TEMfbipWC5P6H3b4y5vAOGMM5QAoobVYlFuaqJf5yiDwSIpNzWRMAkgbBEoAUSV3LREv85RBoOhvXUBQLgiUAKIKslxscpOjAuZXUqLpOzEOCXH0SMJIHwRKAFEnd6ZKSGzS2lobz0AEM4IlACiTkabOHXPSDK7DElS94wkZbSJM7sMAPALgRJAVOrVLkVJthjTHn1bJCXbYtSrHbuTAMIfgRJAVIqxWjSoU7rMaqy2WKSBndIVYw2V05wA4DsCJYColZ5g05CcjFbfpbRIGpKTwc04ACIGg80BRL2SukYt3VEhw1DQm3Wslr1hMjvJt9t0ACAUESgBQFJlg1MrdlaqzukO2nsk22I0sFM6O5MAIg6BEgD2cXsMrS+v0caKOlkUmN3KpnW6ZySpV7sUzkwCiEgESgA4SEW9Q9+X1ajE7vA5WDa9LjsxTr0zUxgNBCCiESgBoBm1DpeKquwqqrbL5dn7rbK5gOlxu2SxxshisSjWuvfO8Ny0RG7AARAVCJQAcBQew1CNw6WqBqcqG5yqbnTK6TbkMQxZLRbZYiwqWv+d/vXPf+jVf/5D6YkJspo1jwgATECgBIAAWLJkiYYOHapVq1apf//+ZpcDAK2KOZQAEAD9+/dXbGysli9fbnYpANDqCJQAEABt2rTRKaecQqAEEJUIlAAQIHl5eQRKAFGJQAkAAZKXl6cffvhBVVVVZpcCAK2KQAkAAZKXlydJWrlypcmVAEDrIlACQIB0795dqampPPYGEHUIlAAQIFarVYMGDSJQAog6BEoACKCmxhxG/AKIJgRKAAigvLw8lZaWauvWrWaXAgCthkAJAAHU1JjDY28A0YRACQABlJWVpdzcXAIlgKhCoASAAMvLy9OKFSvMLgMAWg2BEgACLC8vT6tXr5bT6TS7FABoFQRKAAiwvLw8NTQ0aN26dWaXAgCtgkAJAAF26qmnymazcY4SQNQgUAJAgCUkJKhv374ESgBRg0AJAEHQNOAcAKIBgRIAgmDQoEHasGGDKisrzS4FAIKOQAkAQdA04HzlypUmVwIAwUegBIAgOPHEE5WWlsZjbwBRgUAJAEFgtVo1aNAgAiWAqECgBIAgaWrMMQzD7FIAIKgIlAAQJHl5eSorK1NRUZHZpQBAUBEoASBIBg0aJEk89gYQ8QiUABAkWVlZOuGEEwiUACIegRIAgogB5wCiAYESAIIoLy9PX3/9tRwOh9mlAEDQECgBIIjy8vLU2NiotWvXml0KAAQNgRIAgqhfv36y2Ww89gYQ0QiUABBECQkJ6tevn1asWGF2KQAQNARKAAgyGnMARDoCJQAEWV5enjZu3KjKykqzSwGAoCBQAkCQ5eXlSRKPvQFELAIlAARZt27dlJGRwWNvABEr1uwCACDSWSwW3XzzzerVq5fZpQBAUFgMwzDMLgIAooHT6VRsbKwsFovZpQBAQBEoAQAA4BfOUAIAAMAvBEoAAAD4hUAJAAAAvxAoAcAEhmEw6BxAxCBQAoAJHA6HrrzySjU2NppdCgD4jUAJACaIj4/Xli1btGnTJrNLAQC/ESgBwCR5eXmaN2+e2WUAgN8IlABgkokTJyojI8PsMgDAbww2BwCTGIYhwzBktfKzPYDwRqAEAACAX/ixGABMwM/yACIJgRIAWllDQ4O++eYbs8sAgIAhUAJAK3vkkUc0cuRIdikBRAwCJQC0skGDBqm8vFybN282uxQACAgCJQC0skGDBkmSli9fbnIlABAYBEoAaGUZGRk68cQTCZQAIgaBEgBMkJeXR6AEEDEIlABggry8PK1Zs0aNjY1mlwIAfiNQAoAJ8vLy5HA4tGbNGrNLAQC/ESgBwAR9+/ZVfHw8j70BRAQCJQCYIC4uTqeeeiqBEkBEIFACgElozAEQKQiUAGCSvLw8bd68WWVlZWaXAgB+IVACgEmaBpyvWLHC5EoAwD8ESgAwyQknnKDMzEweewMIewRKADCJxWLRoEGDCJQAwh6BEgBMlJeXpxUrVsgwDLNLAQCfESgBwER5eXmqrKzUpk2bzC4FAHxGoAQAEzU15vDYG0A4I1ACgInS09PVvXt3AiWAsEagBACTMeAcQLgjUAKAyfLy8rRmzRo1NDSYXQoA+IRACQAmy8vLk9Pp1Jo1a8wuBQB8QqAEAJOdcsopio+P57E3gLBFoAQAk8XFxal///4ESgBhi0AJACGAxhwA4YxACQAhIC8vT1u2bFFpaanZpQBAixEoASAE5OXlSZJWrFhhciUA0HIESgAIAV27dlVWVhaPvQGEJQIlAIQAi8XCOUoAYYtACQAhIi8vTytWrJDH4zG7FABoEQIlAISIvLw8VVVV6ccffzS7FABoEQIlAISIgQMHShKPvQGEHQIlAISItLQ09ezZk0AJIOwQKAEghAwaNIhACSDsECgBIITk5eVp7dq1amhoMLsUADhmBEoACCF5eXlyuVz6+uuvzS4FAI4ZgRIAQsgpp5yihIQEHnsDCCuxZhcAAPifmNhYnXXehSpu9GhNcbWqGp1yuQ15DENWi0WxMRalxduUlmBTerxNKfGxslosZpcNIMpZDMMwzC4CAKJdrcOloiq7iqrtcnkMGfsC5OG+QVsk78djrRblpiYqNy1RyXHsEQAwB4ESAExUUe/Q92U1KrE7DgiKLdH0uuzEOPXOTFFGm7jAFgkAR0GgBAATuD2G1pfXaGNFnc9B8mBN63TPSFKvdimKsfIoHEDrIFACQCurbHBqxc5K1TndQXuPJFuMBnVKV3qCLWjvAQBNCJQA0IpK6hq1dEeFDCMwu5LNsUiyWKQhORnKTooP4jsBAIESAFpNSV2jlmyvCGqQPJhFUv5xhEoAwcUcSgBoBZUNzr07k638voakpTsqVNngbOV3BhBNCJQAEGRuj6EVOytl1vMgjyGt3Fkpt4cHUgCCg0AJAEG2vrxGdU53q+9O7q/W6db68hoTKwAQyQiUABBEFfUObayoM7sMSdLGijpV1DvMLgNABCJQAkAQfV9Wo1CZBmnR3noAINAIlAAQJLUOl0rsDlMfde/PkFRid6jW4TK7FAARhotfASBIiqrsft+Cs3vbVr076zmtXfqZKkuKFWuzqUv3nhry6/N05rgJik9o06L1LPvq6pPd1o+qAOBABEoACAKPYaio2u5XmFy1aJ4eu/EPssXFa9j5Y9XlxJ5yOR1av2qF5jx6v37+cYOuuf/RFq1pSCqqtuukrBRZLaHyMB5AuCNQAkAQ1DS65PJjTE/x9m164uZrlNXpON378ptKz27v/dyvJ1ymXT8VadXi+T6t7fIYqnG4lBrPtYwAAoMzlAAQBJWN/g0Sf3fWc2qw1+na6Y8fECabdDw+V+dMmuzz+lUMOgcQQARKAAiCqganX93dXy38VO07H6+e/QcGrKYmFombcwAEFIESAIKgqtHp8/lJe22NKop3qUv3ngGtqYkhqdrPHVQA2B+BEgCCwOX2/fxkfe3eWZFtkpIDVc4hnH7UBwAHI1ACQBB4/Li4u01yiiSpvq42UOUcwp/6AOBgBEoACAJ/RvIkJqcoI7uDfv5xQwArOhAjgwAEEoESAIIgNsa/wDZg+Gjt3rZVG77+KkAVHcjmZ30AsD8CJQAEQVq8za8u7wsmX6uExEQ9d9etqiorPeTzu7dt1YezZ/m0tkViBiWAgGKwOQAEQVqCza9bcjp06aobH3tWT9x0jW74f8P23ZTTQy6nUxu+/kpLP/5QIy4c59PahqT0BAIlgMCxGAYnswEg0KobnJr/U5nf6+zcukXvvThT65Z+poqSYtni4nR8j17KH3O+zhw3Qba4eJ/WHdU1k11KAAFDoASAIPAYhj7cVOzX9YvB4nI0qm+cQz16dDe7FAARgjOUABAEVotFuamJUoj9zO7xePTpG3PVs2cPnXbaaXr++edVWVlpdlkAwhyBEgCC4Ouvv9bj026TQmw8j9Vq1YN/vlGvvfaa0tPTdd1116ljx44aN26c/u///k8ul8vsEgGEIQIlAATQl19+qXPPPVf9+/fX5/M/lbOy1K9u70CySMpOjFNm22SNHz9eH330kbZv364ZM2bohx9+0DnnnKOcnBzdfPPNWrt2rdnlAggjBEoACIDFixfrzDPP1GmnnaZNmzZpzpw52rBhg87s29Ovbu9AMiT1zkw54GMdO3bULbfcorVr12r16tW6+OKLNXfuXPXr10/9+vXTE088oeLiYnMKBhA2aMoBAB8ZhqFPP/1U06dP1+eff66+fftq2rRp+s1vfiOr9X8/r39bukcbK+pMrHSv7hlJOjmr7VG/zul06uOPP1ZhYaE++OADud1u/epXv1JBQYHOPfdcJSQktEK1AMIJO5QA0EKGYej9999XXl6ezj77bDU2Nur999/X119/rbFjxx4QJiWpV7sUJdliTHv0bZGUbItRr3YpR/1aSbLZbDr33HP11ltvadeuXXrqqadUWlqqcePGqWPHjrrmmmu0bNkysR8BoAk7lABwjNxut9555x1Nnz5d69at0+mnn6677rpLo0ePluUozTeVDU4t3lYmM6YIWS3SsC6Zfg8z/+GHHzR79mzNmTNH27dvV/fu3TVp0iRNnDhRXbp0CVC1AMIRgRIAjsLlculf//qXHnjgAf3www8688wzNW3aNJ1xxhktWqekrlFLtle06plKi6T84zKUneTbAPTDcbvdWrhwoQoLC/XOO++ovr5eI0aM0KRJk3TRRRcpOTk5YO8FIDwQKAGgGQ6HQ4WFhXrooYe0ZcsWnXvuuZo6dary8vJ8XrOkrlFLd1TIMBT0YGm1SENyAhsmD1ZTU6O3335bhYWFWrRokZKSknTRRRepoKBAw4cPP+TxP4DIRKAEgIPU19frxRdf1MMPP6wdO3booosu0tSpU9WvX7+ArF/Z4NSKnZWqc7oDst7hJNtiNLBTeqve2b1161bNmTNHs2fP1qZNm9S5c2dNnDhRBQUF6t6dW3mASEagBIB9amtr9fzzz+uxxx5TaWmpLrnkEt15553q3bt3wN/L7TG0vrxGGyvqZFFgdiub1umekaRe7VIUYzWnDcgwDC1btkyFhYV6/fXXVV1drcGDB6ugoEDjx49Xenq6KXUBCB4CJYCoV11draefflp//etftWfPHhUUFOiOO+5Qt27dgv7eFfUOfV9WoxK7w+dg2fS67MQ49c5MUUabuMAW6Yf6+nq9//77mj17tj7++GPFxsbqvPPOU0FBgc4++2zZbK23gwogeAiUAKJWWVmZnnzyST399NNqaGjQ5MmTddttt5nSsVzrcKmoyq6iartc+1rBmwuY+3881rr3zvDctEQlx8W2UrW+2b17t1555RUVFhbqm2++UXZ2tiZMmKCCggL17dvX7PIA+IFACSDq7N69W48//rhmzpwpwzB0zTXX6JZbblHHjh3NLk0ew1CNw6WqBqcqG5yqbnTK6TbkMQxZLRbZYixKjbcpPcGmtASbUuJiZQ2x+8KPxjAMrVmzRrNnz9Yrr7yi0tJS9e3bV5MmTdKECRPUvn17s0sE0EIESgBR4+eff9ajjz6qf/zjH7LZbPrTn/6km266SZmZmWaXFrW4lQeIDARKABFvy5Yteuihh/Tyyy8rJSVFN954o/74xz/SHBJiKioq9Nprr2n27Nlavny50tLSNH78eBUUFGjw4MFHHR4PwDwESgAR64cfftCDDz6oV155Re3atdOtt96qq6++Wikpx3YFIczT3K08l156qY4//nizywNwEAIlgIizbt06zZgxQ2+++aY6deqk2267TZMnT1ZiYqLZpaGFmm7lmT17tt5++23Z7XaNGDFCBQUF3MoDhBACJYCIsXLlSk2fPl3vv/++unbtqjvvvFMFBQWKjw/eTTFoPdzKA4QuAiWAsPfFF19o+vTp+uSTT9S9e3dNmTJFl1xyCTMOI9jWrVs1d+5cFRYWcisPEAIIlADCkmEYWrBgge6//34tXrxYJ598sqZNm6axY8cqJibG7PLQSriVBwgNBEoAYcUwDH300UeaPn26vvzySw0YMEDTpk3TeeedxyPPKFdfX68PPvhAhYWF+uSTTxQTE8OtPEArIVACCAsej0f//ve/NX36dK1Zs0ZDhgzRXXfdpbPPPptxMjhEc7fyTJo0Sf369TO7PCDiECgBhDSXy6U33nhDM2bM0Pfff6+RI0dq2rRpGj58OEESR2UYhtauXavCwkLvrTynnHKKCgoKuJUHCCACJYCQ5HQ6NWfOHD344IPatGmTxowZo6lTp2rIkCFml4Ywdbhbec4++2wVFBTovPPO41YewA8ESgAhpaGhQS+99JIeeughbdu2TRdeeKGmTp2qAQMGmF0aIkhFRYVef/11FRYWcisPEAAESgAhoa6uTn//+9/16KOPqri4WOPHj9eUKVN08sknm10aItzBt/Js3rxZubm5hEqgBQiUAEy1Z88ePfvss3riiSdUWVmpiRMn6s4772SWIFqd2+3WqlWrNGjQILNLAcIOgRKAKerr6/Xwww/rySeflN1u1+WXX67bb79dXbt2Nbs04LDWrl2rnJwcZWZmml0KEHIY2gbAFC6XS88//7x+//vfa8uWLZo5cyZhEiHL4/Fo3bp16tGjh3r16qVPPvlE7McA/8MOJQBTeDwe2e12JScnm10KcMzsdrs+/vhjlZSU6PLLL1dcXJzZJQEhgUAJIKg8Ho9+/PFHnXjiidxkg4i1a9cu3XDDDZo0aRK38iAq8d0dQFC43W69+OKLSk5O1lNPPSWn02l2SUBQGIahn376ST/88IPOPfdcHXfccbrpppu0Zs0as0sDWg07lAACzjAMPfXUU1q4cKGmTJlC1yyixpo1a7iVB1GJQAkgIAzDOGBuX2Njo+Lj402sCDBP0608s2fP1vvvv8+tPIh4BEoAflm2bJkWLFigqVOnml0KEJKau5Vn0qRJOu200xigjohAoATQYoZhaNGiRZo+fboWLFigk046SUuXLlVKSgp/OQJH8MMPP2jOnDmaM2eOfv75Z5144omaNGmSJk6cqOOPP97s8gCf0ZQD4JgZhqGPP/5Yp59+ukaOHKmKigq99dZbWrdundq2bUuYBI6iZ8+emjFjhrZu3ap58+Zp8ODBevDBB9W1a1eNHDlShYWFqq2tNbtMoMUIlACOyuPx6N1339XAgQP161//Wi6XSx9++KFWr16tiy66iHFAQAtZrVaNGjVKs2fP1u7du/XSSy9Jkn7/+9+rffv2mjRpkubPny+Px2NypcCx4ZE3gGa53W699dZbmjFjhr755hsNGzZMd911l0aOHMluJBAEP/30k+bMmaPCwkJt2rRJnTt31qWXXqqCggL16NHD7PKAZhEoARzC6XTq1Vdf1QMPPKCNGzfq7LPP1tSpU3X66aebXRoQFQzD0LJlyzR79my99tprqq6uVl5engoKCjR+/HhlZGSYXSJwAAIlAK/GxkYVFhbqoYceUlFRkc477zxNmzZNAwcONLs0IGo1NDTo/fffV2FhoT755BPFxMTovPPO06RJk/SrX/2KW3kQEgiUAGS32zVr1iw98sgj2rlzp377299qypQp6tu3r9mlAdjP7t279eqrr6qwsFDr1q1Tdna2LrnkEhUUFKhfv35ml4coRqAEolhNTY1mzpypxx9/XOXl5ZowYYLuvPNO9ezZ0+zSABzF4W7lmTRpkiZMmKAOHTqYXR6iDIESiEJVVVV6+umn9be//U01NTX6/e9/r9tvv12/+MUvzC4NQAs5nU598sknKiws5FYemIZACUSR0tJS/e1vf9Mzzzwjh8OhK6+8Un/+85/VuXNns0sDEAAH38qTmpqq8ePHq6CgICxv5fEYhmoaXapsdKqqwamqRqdcbkMew5DVYlFsjEVp8TalJdiUHm9TSnysrGH27xgpCJRAFNi1a5cee+wxPf/887JYLLrmmmt0yy238FgMiGAbNmzQ7Nmzw/JWnlqHS0VVdhVV2+Xy7I0pFkmHCyz7fzzWalFuaqJy0xKVHBfbStVCIlACEW3btm165JFHNGvWLMXHx+v666/XDTfcoMzMTLNLA9BKPB6PFi5cqMLCQr399tuy2+0aPny4CgoKNHbsWCUnJ5tdoldFvUPfl9WoxO5oNkAeTdPrshPj1DszRRlt4gJbJA6LQAlEoM2bN+vBBx9UYWGh2rZtq5tuukl//OMflZaWZnZpAExUW1urt99+W4WFhVq4cKESExN10UUXqaCgQCNGjDDt1iu3x9D68hptrKjzOUgerGmd7hlJ6tUuRTFWHoUHE4ESiCDr16/XAw88oFdffVVZWVm69dZbdfXVV4fUDgSA0NB0K8/s2bP1448/mnYrT2WDUyt2VqrO6Q7aeyTZYjSoU7rSE5jZGSwESiACrFmzRjNmzNDbb7+tnJwc3X777briiivUpk0bs0sDEOIMw9CXX36pwsLCVr+Vp6SuUUt3VMgwArMr2RyLJItFGpKToeyk+CC+U/QiUAJhbMWKFZo+fbo++OAD5ebm6s4779SkSZMUH883TAAt13Qrz+zZs/Xxxx8rJiZG5557rgoKCgJ+K09JXaOWbK8IapA8mEVS/nGEymAgUAIKv9EUn332maZPn65PP/1UPXr00NSpU3XxxRcrNpauRgCB0dytPJMmTVK/fv38GkFU2eDU4m1l8piQQKwWaViXTB5/BxiBElEtnEZTGIahefPm6f7779fnn3+uPn36aNq0abrooosUExPTKjUAiE4H38rTp08fFRQU+HQrj9tjaN7WUtmd7lbdndxfsi1Go7pm0agTQARKRKVwGk1hGIY+/PBDTZ8+XStWrNAvf/lL3XXXXTrnnHNM68gEEJ2au5Vn0qRJOv/884/pVp5vS/doY0VdK1R7ZN0zknRyVluzy4gYBEpElXAaTeHxePTOO+9o+vTpWrt2rfLz83XXXXfprLPOCrvbLgBEnqZbeWbPnq0vv/zymG7lqah3aNG2chOqPbzhXdoxpzJACJSIGuEymsLlcum1117TAw88oPXr12v06NGaNm2azjjjDIIkgJB0rLfyfPFzuUrtDtMede/PIikrMU5DO7czu5SIQKBEVAiH0RQOh0Nz5szRgw8+qM2bN+ucc87R1KlTNXjw4OAUCwAB1nQrz+zZs/XWW28dcCvPr8+/UEtK7GaXeIizcrO4pjEAOICFiNc0msIT5DAp7V3fY0hLtleopK7xmF5TX1+vZ599Vt26ddPkyZPVr18/rV69Wh988AFhEkBYsVqtGjVqlAoLC1VcXKyXX35ZFotFl112maY+9pQ8npY/IVrwzuu6qGcnbfpm7WE//5eJF+nGc0f4VK9FUlFV6IXccESgRESrbHDu3Zls5fc1JC3dUaHKBmezX1NXV6fHH39cJ5xwgq6//nqdccYZ+vbbb/XWW2/p1FNPbb1iASAIkpOTVVBQoAULFqioaKvOvniSrNbQmkhhSCqqtsvDw1q/ESgRsdweQyt2Vsqs7xMeQ1q5s1LugwatVVdX64EHHtDxxx+vO+64Q2PGjNEPP/yguXPn6qSTTjKnWAAIovQOnRRjC83mF5fHUI3DZXYZYY9DA4hY68trgtqAcyxqnW6tL6/RyVltVV5erieffFJPPfWU6uvrNXnyZN12220HHFgHgEhU2dj805pQUNXgVGo8g879QaBERKqod4TEnDNJ2lhRp7df+oceve9ueTweXX311br11lvVqVMns0sDgFZR1eD0e1SbvXaP9lQeOnLI5fIvrFq093jU8al+LRP1CJSISN+X1QRszqS/3C6XlN1F119/vW666SZlZWWZXRIAtKqqRqff34/vvWx8s5/rfGIPn9c1JFWH+A5qOCBQIuLUOlwqsTvMLsMrJjZWfU4bqrMuuZDRFACiksvt/4/3V/7lAXXsesIhHy98+D6fusf35wxAfdGOv90QcYqq7D7tTi5453U9O+UmPfzmf9StT99DPv+XiRdpT1WF/vbBwhbX1DSaok8213wBiD6B6KLu1ufUw35vTm6bqj1VFX6tTZe3/+jyRkTxGIaKqu0h8ah7f4ymABDNrCF+y1eo1xcOCJSIKDWNLrk8oRnaGE0BIFrFxoR2YLOFeH3hgECJiBIOoykAINqkxdsUqpHNIjEyKAA4Q4mIwmgKAAg9aQm2kDuK1MSQlJ5AoPQXgRIRhdEUABB60kN8BzCNQOk3AiUiCqMpACD0pMTHKtZq8emM+8jfjNfI3zT/g/59c972pzTFWi1KYaSb3/gviIjCaAoACD1Wi0W5qYnaVFkXUo++LZJyUxPp8g4AmnIQUUL9m0Ko1wcAwZKblhhSYVLaexQpNy3R7DIiAoESEYXRFAAQmpLjYpWdGBcy3d4WSdmJcdxgFiAESkQURlMAQOjqnZkSMruUhvbWg8AgUCKiMJoCAEJXRps4dc9IMrsMSVL3jCRltIkzu4yIQaBERGE0BQCEtl7tUpRkizHtaZJFUrItRr3asTsZSBbDoO0UkcNjGPpwU3FIXr8Ya7XonG7tacwBEPUqG5xavK1MZnyrtlqkYV0yeWIUYOxQIqI0jaYItcjGaAoA+J/0BJuG5GS0+vdqi6QhORmEySAgUCLiMJoCAEJfdlK88o/LkNWiVgmWVouUf1yGspPiW+Hdog+BEhGH0RQAEB6yk+I1rEumEm0xQX2fZFuMhnXJJEwGEYESEYnRFAAQHtITbBrdNcvb/R2ozYCmdbpnJGlU1ywecwcZTTmIWN+W7tHGijqzy1D3jCSdnNXW7DIAIORV1Dv0fVmNSuwOWSSfNgaaXpedGKfemSmMBmolBEpELLfH0LytpbI73absVlokJdliNKprlmKsofIAHgBCX63DpaIqu4qq7d6pHc0FzP0/Hmvd25iZm5bIMaNWRqBERGM0BQCEL49hqMbhUlWDU5UNTlU3OuV0G/IYhqwWi2wxFqXG25SeYFNagk0pcbFM0zAJgRIRr6SuUUu2V7TqLqVFdBMCAKIHgRJRoaSuUUt3VMgwfDuT0xJWy945Z4RJAEC0IFAialQ2OLViZ6XqnO6gvUeyLUYDO6XzmBsAEFUIlIgqbo+h9eU12lhR53MH4cGa1umekaRe7VJowAEARB0CJaISoykAAAgcAiWiGqMpAADwH4ESEKMpAADwB4ESAAAAfuEubwAAAPiFQAkAAAC/ECgBAADgFwIlAAAA/EKgBAAAgF8IlAAAAPALgRIAAAB+IVACAADALwRKAAAA+IVACQAAAL8QKAEAAOAXAiUAAAD8QqAEAACAXwiUAAAA8AuBEgAAAH4hUAIAAMAvBEoAAAD4hUAJAAAAvxAoAQAA4BcCJQAAAPxCoAQAAIBfCJQAAADwC4ESAAAAfiFQAgAAwC8ESgAAAPiFQAkAAAC/ECgBAADgFwIlAAAA/EKgBAAAgF8IlAAAAPDL/wegyTxon5m7BQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#This Ends the Implementation of MolGAN Research Paper\n",
        "\n",
        "**To do Further\n",
        "```\n",
        "Do Parrallel Programing\n",
        "1.Study Multiplication and addition code of cuda , and apply in the program above .\n",
        "```"
      ],
      "metadata": {
        "id": "fppmTc6svibw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CGxKMlPTYWjV"
      }
    }
  ]
}