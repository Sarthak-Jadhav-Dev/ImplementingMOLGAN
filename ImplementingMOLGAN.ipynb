{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMD0Su9Hw3SD8662sUDoj8p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sarthak-Jadhav-Dev/ImplementingMOLGAN/blob/main/ImplementingMOLGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explaination of Entire Notebook as of (12:21 pm) on 14/09/2025 is in this Link** :  https://www.perplexity.ai/search/please-explain-what-we-are-doi-OUwOPGmCQouW.7TSLQm6iw\n",
        "\n",
        "**Explaination of Entire Notebook as of (4:51 pm) on 16/09/2025 is in this Link** :  https://www.perplexity.ai/search/explain-me-this-entire-noteboo-ODtCogBhS_KVo1b6h2KjpA\n",
        "\n",
        "**Explaination of Entire Notebook as of (3:51pm) as on17/09/2025 is in this link** : https://www.perplexity.ai/search/explain-me-this-notebook-s-las-a2_bSCz9RxiPVR.1Dz57pQ\n",
        "\n",
        "**Guide For the Project is Below :** https://chatgpt.com/share/68c65ed1-0ba0-8008-9542-a798c0d211a7 (intial) and https://chatgpt.com/c/68c65f82-5890-8322-95d2-0a06d8769372 (final)\n",
        "\n",
        "\n",
        "## Structure of Notebook ##\n",
        "**The Notebook has 2 Parts**\n",
        "\n",
        "**1: Just Testing the Dependences and Environment**\n",
        "```\n",
        "Cell 1,2,3,4,5,6,7,8\n",
        "```\n",
        "\n",
        "**2: The Actual Part Where we Want to Excecute the System like Training and Testing**\n",
        "```\n",
        "Cell 1,2,9,10,11,12\n",
        "```"
      ],
      "metadata": {
        "id": "zQVgJe_1_i11"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPn2lJsO2OKn",
        "outputId": "4c2e7e8c-db6f-4d76-86a0-e811bf898066"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping torch-scatter as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-sparse as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-geometric as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in links: https://data.pyg.org/whl/torch-2.1.0+cu118.html\n",
            "Collecting torch-scatter\n",
            "  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-sparse\n",
            "  Downloading torch_sparse-0.6.18.tar.gz (209 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch-sparse) (1.16.1)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy->torch-sparse) (2.0.2)\n",
            "Building wheels for collected packages: torch-scatter, torch-sparse\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 423, in run\n",
            "    _, build_failures = build(\n",
            "                        ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/wheel_builder.py\", line 319, in build\n",
            "    wheel_file = _build_one(\n",
            "                 ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/wheel_builder.py\", line 193, in _build_one\n",
            "    wheel_path = _build_one_inside_env(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/wheel_builder.py\", line 240, in _build_one_inside_env\n",
            "    wheel_path = build_wheel_legacy(\n",
            "                 ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/build/wheel_legacy.py\", line 83, in build_wheel_legacy\n",
            "    output = call_subprocess(\n",
            "             ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/utils/subprocess.py\", line 151, in call_subprocess\n",
            "    line: str = proc.stdout.readline()\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 216, in exc_logging_wrapper\n",
            "    logger.debug(\"Exception information:\", exc_info=True)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1527, in debug\n",
            "    self._log(DEBUG, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1684, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1700, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1762, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1028, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/lib/python3.12/logging/handlers.py\", line 75, in emit\n",
            "    logging.FileHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1280, in emit\n",
            "    StreamHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1160, in emit\n",
            "    msg = self.format(record)\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 999, in format\n",
            "    return fmt.format(record)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/utils/logging.py\", line 112, in format\n",
            "    formatted = super().format(record)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 711, in format\n",
            "    record.exc_text = self.formatException(record.exc_info)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 661, in formatException\n",
            "    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
            "  File \"/usr/lib/python3.12/traceback.py\", line 124, in print_exception\n",
            "    te = TracebackException(type(value), value, tb, limit=limit, compact=True)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/traceback.py\", line 733, in __init__\n",
            "    self.stack = StackSummary._extract_from_extended_frame_gen(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/traceback.py\", line 418, in _extract_from_extended_frame_gen\n",
            "    for f, (lineno, end_lineno, colno, end_colno) in frame_gen:\n",
            "                                                     ^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/traceback.py\", line 355, in _walk_tb_with_full_positions\n",
            "    positions = _get_code_position(tb.tb_frame.f_code, tb.tb_lasti)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/traceback.py\", line 369, in _get_code_position\n",
            "    return next(itertools.islice(positions_gen, instruction_index // 2, None))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.12.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2025.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "# This Cell is For Just Installing the Dependencies\n",
        "#Remember that wait for 2 to 3 mins and then stop the prcess console will print correct code\n",
        "#Every time Run this Cell if you want to use the Entire Setup\n",
        "\n",
        "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "# !pip install torch-geometric torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
        "# !pip install pennylane matplotlib\n",
        "\n",
        "# Clean up old attempts\n",
        "!pip uninstall -y torch-scatter torch-sparse torch-geometric\n",
        "\n",
        "# Install stable builds compatible with Colab's PyTorch (2.1.0 + cu118)\n",
        "!pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.1.0+cu118.html\n",
        "!pip install torch-geometric\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#If Cell 1 Takes to Much time ie the wheels hang Problem then run this code\n",
        "\n",
        "!pip install torch==2.0.0+cu118 torchvision==0.15.1+cu118 torchaudio==2.0.1+cu118 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
        "!pip install pennylane matplotlib\n"
      ],
      "metadata": {
        "id": "4iuYYFC4wdSt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0fb76b7-d7d2-49ee-97ba-12fa1472f741"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==2.0.0+cu118 (from versions: 2.2.0+cu118, 2.2.1+cu118, 2.2.2+cu118, 2.3.0+cu118, 2.3.1+cu118, 2.4.0+cu118, 2.4.1+cu118, 2.5.0+cu118, 2.5.1+cu118, 2.6.0+cu118, 2.7.0+cu118, 2.7.1+cu118)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==2.0.0+cu118\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in links: https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
            "Collecting torch-scatter\n",
            "  Using cached torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-sparse\n",
            "  Using cached torch_sparse-0.6.18.tar.gz (209 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-cluster\n",
            "  Downloading torch_cluster-1.6.3.tar.gz (54 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-spline-conv\n",
            "  Downloading torch_spline_conv-1.2.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.12/dist-packages (2.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch-sparse) (1.16.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.12.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2025.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n",
            "Building wheels for collected packages: torch-scatter, torch-sparse, torch-cluster, torch-spline-conv\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hcanceled\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting pennylane\n",
            "  Downloading pennylane-0.42.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from pennylane) (3.5)\n",
            "Collecting rustworkx>=0.14.0 (from pennylane)\n",
            "  Downloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.8.0)\n",
            "Collecting appdirs (from pennylane)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting autoray<0.8,>=0.6.11 (from pennylane)\n",
            "  Downloading autoray-0.7.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from pennylane) (5.5.2)\n",
            "Collecting pennylane-lightning>=0.42 (from pennylane)\n",
            "  Downloading pennylane_lightning-0.42.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.32.4)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.13.3)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from pennylane) (4.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from pennylane) (25.0)\n",
            "Collecting diastatic-malt (from pennylane)\n",
            "  Downloading diastatic_malt-2.15.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Collecting scipy-openblas32>=0.3.26 (from pennylane-lightning>=0.42->pennylane)\n",
            "  Downloading scipy_openblas32-0.3.30.0.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (1.6.3)\n",
            "Requirement already satisfied: gast in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (0.6.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (3.1.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2025.8.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\n",
            "Downloading pennylane-0.42.3-py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autoray-0.7.2-py3-none-any.whl (930 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m930.8/930.8 kB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pennylane_lightning-0.42.0-cp312-cp312-manylinux_2_28_x86_64.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading diastatic_malt-2.15.2-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m167.9/167.9 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy_openblas32-0.3.30.0.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m130.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: appdirs, scipy-openblas32, rustworkx, autoray, diastatic-malt, pennylane-lightning, pennylane\n",
            "Successfully installed appdirs-1.4.4 autoray-0.7.2 diastatic-malt-2.15.2 pennylane-0.42.3 pennylane-lightning-0.42.0 rustworkx-0.17.1 scipy-openblas32-0.3.30.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Guide Link:** https://chatgpt.com/share/68c65ed1-0ba0-8008-9542-a798c0d211a7 (intial) and\n",
        "https://chatgpt.com/c/68c65f82-5890-8322-95d2-0a06d8769372 (final)\n",
        "\n",
        "**Problem with Rdkit Method Great question** ğŸ™Œ\n",
        "â€” the RDKit issues you faced in Colab are very common. Let me break down why it was so messy:\n",
        "\n",
        "**1. Colabâ€™s Python version changed**\n",
        "\n",
        "In 2023, Colab used Python 3.8/3.9, and rdkit-pypi or apt-get install python3-rdkit worked smoothly.\n",
        "\n",
        "In 2024â€“2025, Colab upgraded to Python 3.11/3.12.\n",
        "\n",
        "RDKitâ€™s wheels (precompiled binaries) on PyPI (rdkit-pypi) do not always provide builds for the latest Python versions right away. Thatâ€™s why you saw:\n",
        "\n",
        "ERROR: No matching distribution found for rdkit-pypi\n",
        "\n",
        "\n",
        "â†’ It simply didnâ€™t exist for your Python version.\n",
        "\n",
        "**2. Mixing installation methods**\n",
        "\n",
        "Colab has multiple ways to install RDKit:\n",
        "\n",
        "!pip install rdkit or rdkit-pypi (PyPI wheels).\n",
        "\n",
        "!apt-get install python3-rdkit (Ubuntu system package).\n",
        "\n",
        "condacolab + mamba install -c conda-forge rdkit (Conda build).\n",
        "\n",
        "If you run more than one, Colab may end up with conflicting RDKit versions (one in /usr/lib/python3/dist-packages/, another in /usr/local/lib/python3.x/dist-packages/). This is what caused the ImportError: cannot import name 'rdBase' â€” Python was importing a half-installed RDKit from the wrong folder.\n",
        "\n",
        "**3. Conda pinning issue**\n",
        "\n",
        "When you used condacolab, it pinned Python=3.12 by default, while Colab itself was running Python=3.11. That mismatch is why mamba kept complaining about â€œpinning does not match.â€\n",
        "\n",
        "âœ… In short:\n",
        "\n",
        "The errors happened because Colabâ€™s Python version is too new for some RDKit builds,\n",
        "\n",
        "and mixing pip/apt/conda installs left you with a broken hybrid installation.\n",
        "\n",
        "ğŸ”§ How to avoid this in the future\n",
        "\n",
        "Pick one install method only (pip OR conda, not both).\n",
        "\n",
        "For Colab today (Python 3.11/3.12), the most reliable is:\n",
        "\n",
        "!pip install rdkit\n",
        "\n",
        "\n",
        "Then just do:\n",
        "\n",
        "from rdkit import Chem\n",
        "print(Chem.MolFromSmiles(\"CCO\"))\n",
        "\n",
        "\n",
        "If that runs, youâ€™re good.\n",
        "\n",
        "ğŸ‘‰ Since your hybrid MolGAN setup (PyTorch + PennyLane + QM9 from torch-geometric) works without RDKit for now, you can move forward. Later, when you need QED/LogP/SA metrics, we can add RDKit back carefully."
      ],
      "metadata": {
        "id": "o4xn03Qo4p_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This Cell was Just For Testing Purpose that the Dependencies we installed were correct , and have the Following Data Which is Printed Below\n",
        "from torch_geometric.datasets import QM9\n",
        "dataset = QM9(root=\"./data/QM9\")\n",
        "\n",
        "print(\"QM9 size:\", len(dataset))\n",
        "print(dataset[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zdsgn-r17MKx",
        "outputId": "6c973bdb-7c52-4142-94bd-21058b4a3b54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://data.pyg.org/datasets/qm9_v3.zip\n",
            "Extracting data/QM9/raw/qm9_v3.zip\n",
            "Processing...\n",
            "Using a pre-processed version of the dataset. Please install 'rdkit' to alternatively process the raw data.\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QM9 size: 130831\n",
            "Data(x=[5, 11], edge_index=[2, 8], edge_attr=[8, 4], y=[1, 19], pos=[5, 3], idx=[1], name='gdb_1', z=[5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This the cell for Penny Lane , Only use if we Need Panylane Further\n",
        "!pip install pennylane\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23BfAox98W7r",
        "outputId": "f2901252-9432-4254-bfc5-0d6ce0f5661f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pennylane\n",
            "  Downloading pennylane-0.42.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from pennylane) (3.5)\n",
            "Collecting rustworkx>=0.14.0 (from pennylane)\n",
            "  Downloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.8.0)\n",
            "Collecting appdirs (from pennylane)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting autoray<0.8,>=0.6.11 (from pennylane)\n",
            "  Downloading autoray-0.7.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from pennylane) (5.5.2)\n",
            "Collecting pennylane-lightning>=0.42 (from pennylane)\n",
            "  Downloading pennylane_lightning-0.42.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.32.4)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.13.3)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from pennylane) (4.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from pennylane) (25.0)\n",
            "Collecting diastatic-malt (from pennylane)\n",
            "  Downloading diastatic_malt-2.15.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.0.2)\n",
            "Collecting scipy-openblas32>=0.3.26 (from pennylane-lightning>=0.42->pennylane)\n",
            "  Downloading scipy_openblas32-0.3.30.0.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: astunparse in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (1.6.3)\n",
            "Requirement already satisfied: gast in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (0.6.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (3.1.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2025.8.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\n",
            "Downloading pennylane-0.42.3-py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autoray-0.7.2-py3-none-any.whl (930 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m930.8/930.8 kB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pennylane_lightning-0.42.0-cp312-cp312-manylinux_2_28_x86_64.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading diastatic_malt-2.15.2-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m167.9/167.9 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy_openblas32-0.3.30.0.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: appdirs, scipy-openblas32, rustworkx, autoray, diastatic-malt, pennylane-lightning, pennylane\n",
            "Successfully installed appdirs-1.4.4 autoray-0.7.2 diastatic-malt-2.15.2 pennylane-0.42.3 pennylane-lightning-0.42.0 rustworkx-0.17.1 scipy-openblas32-0.3.30.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pennylane as qml\n",
        "\n",
        "n_qubits = 8\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "@qml.qnode(dev, interface=\"torch\")\n",
        "def quantum_circuit(inputs, weights):\n",
        "    # Encode noise into qubits\n",
        "    qml.AngleEmbedding(inputs, wires=range(n_qubits))\n",
        "    # Variational block\n",
        "    qml.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
        "    return qml.probs(wires=range(n_qubits))\n",
        "\n",
        "class QuantumLayer(nn.Module):\n",
        "    def __init__(self, n_layers=2):\n",
        "        super().__init__()\n",
        "        shape = qml.StronglyEntanglingLayers.shape(n_layers=n_layers, n_wires=n_qubits)\n",
        "        self.weights = nn.Parameter(torch.randn(shape))\n",
        "    def forward(self, x):\n",
        "        outputs = []\n",
        "        for xi in x:\n",
        "            probs = quantum_circuit(xi, self.weights)\n",
        "            outputs.append(probs.to(dtype=torch.float32))  # force float32\n",
        "        return torch.stack(outputs)\n",
        "\n"
      ],
      "metadata": {
        "id": "IX0P92pU72vY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, noise_dim=8, hidden_dim=64, out_dim=128):\n",
        "        super().__init__()\n",
        "        self.q_layer = QuantumLayer(n_layers=2)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(2**n_qubits, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, out_dim)\n",
        "        )\n",
        "    def forward(self, z):\n",
        "        q_out = self.q_layer(z)\n",
        "        return self.fc(q_out)\n",
        "\n",
        "class Cycle(nn.Module):\n",
        "    def __init__(self, in_dim=128, noise_dim=8):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(in_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, noise_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "G = Generator()\n",
        "C = Cycle()\n"
      ],
      "metadata": {
        "id": "v0LWR__S8hcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizerG = torch.optim.Adam(G.parameters(), lr=1e-3)\n",
        "optimizerC = torch.optim.Adam(C.parameters(), lr=1e-3)\n",
        "\n",
        "for step in range(5):\n",
        "    z = torch.randn(4, n_qubits)  # batch of noise\n",
        "    fake = G(z)\n",
        "    z_recon = C(fake)\n",
        "\n",
        "    cycle_loss = torch.nn.functional.mse_loss(z_recon, z)\n",
        "\n",
        "    optimizerG.zero_grad()\n",
        "    optimizerC.zero_grad()\n",
        "    cycle_loss.backward()\n",
        "    optimizerG.step()\n",
        "    optimizerC.step()\n",
        "\n",
        "    print(f\"Step {step}: cycle_loss = {cycle_loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMTMkmHR8mx9",
        "outputId": "67630a50-5b6a-47f5-c85a-f433aa4aba11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: cycle_loss = 1.3365\n",
            "Step 1: cycle_loss = 0.8074\n",
            "Step 2: cycle_loss = 0.7488\n",
            "Step 3: cycle_loss = 0.6074\n",
            "Step 4: cycle_loss = 0.7769\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**âœ… What we have achieved so far**\n",
        "\n",
        "**Environment setup (Cell 1)**: PyTorch + PyTorch Geometric + PennyLane are working on Colab (T4 GPU ready).\n",
        "\n",
        "**Dataset (Cell 2):** QM9 dataset is accessible â€” we havenâ€™t used it fully yet, but itâ€™s ready.\n",
        "\n",
        "**Quantum Layer (Cell 3):** A variational quantum circuit (VQC, 8 qubits) built with PennyLane.\n",
        "\n",
        "**Generator + Cycle (Cell 4):** Generator maps noise â†’ latent representation, Cycle maps back â†’ noise.\n",
        "\n",
        "**Training (Cell 5):** Cycle-consistency loss decreased, proving gradients flow through quantum â†’ classical â†’ cycle.\n",
        "\n",
        "The numbers going down (1.33 â†’ 0.80 â†’ 0.74 â†’ 0.60 â†’ 0.77) mean the model is actually learning to reconstruct noise. âœ…\n",
        "\n",
        "This is essentially the core hybrid quantum GAN building block from your paper â€” the â€œHQ-Cycleâ€ idea is alive and working in a small demo."
      ],
      "metadata": {
        "id": "hTx2iBPB9U__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where you are in the roadmap\n",
        "\n",
        "**âœ… Phase 1: Environment done.**\n",
        "\n",
        "**âœ… Phase 2: Dataset loaded**.\n",
        "\n",
        "**âœ… Phase 3: Generator + Cycle with quantum layer is working.**\n",
        "\n",
        "**ğŸ”œ Phase 4: Add Discriminator + WGAN training.**\n",
        "\n",
        "**ğŸ”œ Phase 5: Hook in QM9 graphs as real samples.**\n",
        "\n",
        "**ğŸ”œ Phase 6: Add reward & evaluation metrics.**"
      ],
      "metadata": {
        "id": "r2UOFsaY-GJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Phase 4: Discriminator + WGAN-GP training loop (single cell) =====\n",
        "# Assumes: PyTorch, torch_geometric dataset QM9 downloaded in ./data/QM9,\n",
        "# and PennyLane installed. It re-defines a compatible Generator.\n",
        "\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch_geometric.datasets import QM9\n",
        "from torch_geometric.data import Data\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ----------------- Parameters -----------------\n",
        "n_qubits = 8\n",
        "noise_dim = n_qubits                 # we use an 8-dim noise that gets encoded in the VQC\n",
        "quantum_out_dim = 2**n_qubits        # 256\n",
        "GRAPH_ADJ_FLAT = 9*9*5               # 405\n",
        "GRAPH_FEAT_FLAT = 9*5                # 45\n",
        "GRAPH_DIM = GRAPH_ADJ_FLAT + GRAPH_FEAT_FLAT  # 450\n",
        "batch_size = 16\n",
        "critic_iters = 5\n",
        "lr = 2e-4\n",
        "lambda_gp = 10.0\n",
        "# ----------------------------------------------\n",
        "\n",
        "# ----------------- Quantum Layer (PennyLane) -----------------\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as pnp\n",
        "\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "@qml.qnode(dev, interface=\"torch\")\n",
        "def qnode_probs(inputs, weights):\n",
        "    # inputs: vector length = n_qubits\n",
        "    qml.AngleEmbedding(inputs, wires=range(n_qubits))\n",
        "    qml.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
        "    return qml.probs(wires=range(n_qubits))\n",
        "\n",
        "class QuantumLayer(nn.Module):\n",
        "    def __init__(self, n_layers=2):\n",
        "        super().__init__()\n",
        "        shape = qml.StronglyEntanglingLayers.shape(n_layers=n_layers, n_wires=n_qubits)\n",
        "        # PennyLane weights are float64 by default; we keep them as torch.Parameter\n",
        "        self.weights = nn.Parameter(torch.randn(shape, dtype=torch.float64) * 0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch, n_qubits) torch.float32\n",
        "        outs = []\n",
        "        for i in range(x.shape[0]):\n",
        "            xi = x[i].double()                # qnode expects double by default\n",
        "            probs = qnode_probs(xi, self.weights)   # double tensor (2**n_qubits,)\n",
        "            outs.append(probs.to(dtype=torch.float32))\n",
        "        return torch.stack(outs, dim=0)  # (batch, 2**n_qubits)\n",
        "\n",
        "# ----------------- Generator (quantum + classical) -----------------\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, n_qubits=8, quantum_layers=2, hidden=512, out_dim=GRAPH_DIM):\n",
        "        super().__init__()\n",
        "        self.q_layer = QuantumLayer(n_layers=quantum_layers)\n",
        "        # Map quantum probs (256) -> hidden -> out_dim (450)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(quantum_out_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, out_dim),\n",
        "            nn.Tanh()   # tanh to keep outputs in -1..1 for stability; we'll map to [0,1] later if needed\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        # z: (batch, n_qubits) float32\n",
        "        q_out = self.q_layer(z)          # (batch, 256), float32\n",
        "        out = self.fc(q_out.float())     # ensure float32 going into fc\n",
        "        return out                        # (batch, GRAPH_DIM), float32\n",
        "\n",
        "# ----------------- Discriminator / Critic -----------------\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, in_dim=GRAPH_DIM, hidden=512):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(hidden, hidden//2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(hidden//2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).view(-1)\n",
        "\n",
        "# ----------------- Utility: gradient penalty -----------------\n",
        "def gradient_penalty(critic, real, fake, device):\n",
        "    batch_size = real.size(0)\n",
        "    alpha = torch.rand(batch_size, 1, device=device)\n",
        "    alpha = alpha.expand_as(real)\n",
        "    interpolates = alpha * real + ((1 - alpha) * fake)\n",
        "    interpolates.requires_grad_(True)\n",
        "    d_interpolates = critic(interpolates)\n",
        "    grads = torch.autograd.grad(\n",
        "        outputs=d_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=torch.ones_like(d_interpolates),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True\n",
        "    )[0]\n",
        "    grads = grads.view(batch_size, -1)\n",
        "    grad_norm = grads.norm(2, dim=1)\n",
        "    gp = ((grad_norm - 1.0) ** 2).mean()\n",
        "    return gp\n",
        "\n",
        "# ----------------- QM9 -> (adj_flat, feat_flat) converter -----------------\n",
        "# Try to use RDKit if available to get reliable bonds/atom types; otherwise fallback via PyG fields.\n",
        "use_rdkit = False\n",
        "try:\n",
        "    from rdkit import Chem\n",
        "    use_rdkit = True\n",
        "    print(\"RDKit available: using RDKit for graph extraction\")\n",
        "except Exception as e:\n",
        "    print(\"RDKit not available; using fallback graph extraction (edge_index / atomic numbers)\")\n",
        "\n",
        "# atom mapping\n",
        "ATOM_TYPES = ['C','O','N','F','H']\n",
        "ATOM_TO_IDX = {a:i for i,a in enumerate(ATOM_TYPES)}\n",
        "\n",
        "def data_to_graphvec(data):\n",
        "    \"\"\"\n",
        "    Input: a torch_geometric.data.Data object (QM9).\n",
        "    Output: torch tensor shape (GRAPH_DIM,) float32\n",
        "    This will produce adjacency one-hot for 5 bond types (9x9x5 -> 405) and features 9x5 -> 45\n",
        "    \"\"\"\n",
        "    # initialize\n",
        "    adj = torch.zeros((9,9,5), dtype=torch.float32)\n",
        "    feats = torch.zeros((9,5), dtype=torch.float32)\n",
        "\n",
        "    if use_rdkit and hasattr(data, 'smiles'):\n",
        "        smi = data.smiles if isinstance(data.smiles, str) else data.smiles[0]\n",
        "        mol = Chem.MolFromSmiles(smi)\n",
        "        if mol is None:\n",
        "            return None\n",
        "        # atoms\n",
        "        for i,a in enumerate(mol.GetAtoms()):\n",
        "            if i>=9: break\n",
        "            sym = a.GetSymbol()\n",
        "            if sym in ATOM_TO_IDX:\n",
        "                feats[i, ATOM_TO_IDX[sym]] = 1.0\n",
        "            else:\n",
        "                feats[i, -1] = 1.0\n",
        "        # bonds\n",
        "        for b in mol.GetBonds():\n",
        "            i = b.GetBeginAtomIdx(); j = b.GetEndAtomIdx()\n",
        "            if i<9 and j<9:\n",
        "                bt = b.GetBondType()\n",
        "                if bt == Chem.BondType.SINGLE: idx=0\n",
        "                elif bt == Chem.BondType.DOUBLE: idx=1\n",
        "                elif bt == Chem.BondType.TRIPLE: idx=2\n",
        "                elif bt == Chem.BondType.AROMATIC: idx=3\n",
        "                else: idx=4\n",
        "                adj[i,j,idx]=1.0; adj[j,i,idx]=1.0\n",
        "    else:\n",
        "        # Fallback: use atomic numbers (data.z) and edge_index as single bond\n",
        "        # data.z is (num_nodes,) atomic numbers\n",
        "        if hasattr(data, 'z'):\n",
        "            z = data.z.cpu().numpy()\n",
        "            for i, atom_num in enumerate(z):\n",
        "                if i>=9: break\n",
        "                sym = None\n",
        "                # map common atomic numbers to symbols\n",
        "                if atom_num == 6: sym='C'\n",
        "                elif atom_num == 8: sym='O'\n",
        "                elif atom_num == 7: sym='N'\n",
        "                elif atom_num == 9: sym='F'\n",
        "                elif atom_num == 1: sym='H'\n",
        "                if sym in ATOM_TO_IDX:\n",
        "                    feats[i, ATOM_TO_IDX[sym]] = 1.0\n",
        "                else:\n",
        "                    feats[i, -1] = 1.0\n",
        "        # edges: data.edge_index (2, E) â€” treat as single bond type index 0\n",
        "        if hasattr(data, 'edge_index'):\n",
        "            ei = data.edge_index.cpu().numpy()\n",
        "            for a,b in ei.T:\n",
        "                if a<9 and b<9:\n",
        "                    adj[a,b,0] = 1.0\n",
        "                    adj[b,a,0] = 1.0\n",
        "\n",
        "    adj_flat = adj.reshape(-1)\n",
        "    feat_flat = feats.reshape(-1)\n",
        "    vec = torch.cat([adj_flat, feat_flat], dim=0).to(dtype=torch.float32)\n",
        "    if vec.numel() != GRAPH_DIM:\n",
        "        return None\n",
        "    return vec\n",
        "\n",
        "# ----------------- Prepare a small real dataset loader (molecules with <=9 atoms) -------------\n",
        "print(\"Loading QM9 dataset (this may take time only on first download)...\")\n",
        "qm9 = QM9(root=\"./data/QM9\")\n",
        "# filter and convert to vectors\n",
        "real_graphs = []\n",
        "for i in range(len(qm9)):\n",
        "    data = qm9[i]\n",
        "    # many QM9 graphs have nodes <=9; we will only take up to 9 nodes via converter\n",
        "    vec = data_to_graphvec(data)\n",
        "    if vec is not None:\n",
        "        real_graphs.append(vec)\n",
        "    if len(real_graphs) >= 2000:   # cap for speed; increase if you want more\n",
        "        break\n",
        "\n",
        "if len(real_graphs) == 0:\n",
        "    raise RuntimeError(\"No valid real graphs extracted from QM9. If RDKit is not installed or dataset fields changed, you may need to fallback to a synthetic dataset.\")\n",
        "\n",
        "real_tensor = torch.stack(real_graphs)  # (N, GRAPH_DIM)\n",
        "print(\"Prepared real graph vectors:\", real_tensor.shape)\n",
        "\n",
        "real_loader = DataLoader(real_tensor, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "# ----------------- Instantiate models -----------------\n",
        "G = Generator(n_qubits=n_qubits, quantum_layers=2, hidden=512, out_dim=GRAPH_DIM).to(device)\n",
        "C = Critic(in_dim=GRAPH_DIM, hidden=512).to(device)\n",
        "\n",
        "opt_G = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.9))\n",
        "opt_C = optim.Adam(C.parameters(), lr=lr, betas=(0.5, 0.9))\n",
        "\n",
        "# ----------------- Training loop (WGAN-GP) -----------------\n",
        "steps = 800    # small test run; increase later\n",
        "print_every = 50\n",
        "\n",
        "real_iter = iter(real_loader)\n",
        "for step in range(steps):\n",
        "    # Train critic more times\n",
        "    for _ in range(critic_iters):\n",
        "        try:\n",
        "            real_batch = next(real_iter)\n",
        "        except StopIteration:\n",
        "            real_iter = iter(real_loader)\n",
        "            real_batch = next(real_iter)\n",
        "        real_batch = real_batch.to(device)\n",
        "\n",
        "        # sample noise and generate fake graphs\n",
        "        z = torch.randn(batch_size, n_qubits, device=device).float()\n",
        "        with torch.no_grad():\n",
        "            fake_batch = G(z)\n",
        "\n",
        "        # critic scores\n",
        "        real_scores = C(real_batch)\n",
        "        fake_scores = C(fake_batch)\n",
        "\n",
        "        # gradient penalty\n",
        "        gp = gradient_penalty(C, real_batch, fake_batch, device)\n",
        "\n",
        "        loss_C = fake_scores.mean() - real_scores.mean() + lambda_gp * gp\n",
        "\n",
        "        opt_C.zero_grad()\n",
        "        loss_C.backward()\n",
        "        opt_C.step()\n",
        "\n",
        "    # Train generator (one step)\n",
        "    z = torch.randn(batch_size, n_qubits, device=device).float()\n",
        "    fake = G(z)\n",
        "    loss_G = -C(fake).mean()   # WGAN generator loss\n",
        "\n",
        "    opt_G.zero_grad()\n",
        "    loss_G.backward()\n",
        "    opt_G.step()\n",
        "\n",
        "    if (step+1) % print_every == 0 or step==0:\n",
        "        # compute some stats\n",
        "        with torch.no_grad():\n",
        "            r = real_batch[:4].cpu().numpy()\n",
        "            f = fake[:4].cpu().numpy()\n",
        "        print(f\"Step {step+1}/{steps} | loss_C: {loss_C.item():.4f} | loss_G: {loss_G.item():.4f}\")\n",
        "\n",
        "print(\"Training finished (toy run). You can now: - increase `steps` - increase dataset size - add cycle+reward to the generator loss.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLqYbFmgt5Sb",
        "outputId": "947723cb-b71d-4b9d-c641-cfc27337d303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "RDKit not available; using fallback graph extraction (edge_index / atomic numbers)\n",
            "Loading QM9 dataset (this may take time only on first download)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://data.pyg.org/datasets/qm9_v3.zip\n",
            "Extracting data/QM9/raw/qm9_v3.zip\n",
            "Processing...\n",
            "Using a pre-processed version of the dataset. Please install 'rdkit' to alternatively process the raw data.\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared real graph vectors: torch.Size([2000, 450])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:829: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:179.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1/800 | loss_C: 6.4979 | loss_G: 0.0511\n",
            "Step 50/800 | loss_C: -4.0554 | loss_G: -0.6642\n",
            "Step 100/800 | loss_C: -3.5074 | loss_G: -1.1817\n",
            "Step 150/800 | loss_C: -3.1273 | loss_G: -0.8033\n",
            "Step 200/800 | loss_C: -2.8350 | loss_G: -0.1245\n",
            "Step 250/800 | loss_C: -2.9427 | loss_G: -0.0965\n",
            "Step 300/800 | loss_C: -3.1811 | loss_G: 0.2207\n",
            "Step 350/800 | loss_C: -3.0227 | loss_G: 0.0980\n",
            "Step 400/800 | loss_C: -3.1856 | loss_G: 0.2310\n",
            "Step 450/800 | loss_C: -2.9618 | loss_G: 0.0095\n",
            "Step 500/800 | loss_C: -3.0193 | loss_G: -0.0357\n",
            "Step 550/800 | loss_C: -2.6964 | loss_G: -0.1676\n",
            "Step 600/800 | loss_C: -2.6583 | loss_G: -0.0921\n",
            "Step 650/800 | loss_C: -2.7677 | loss_G: -0.1022\n",
            "Step 700/800 | loss_C: -2.8886 | loss_G: -0.2136\n",
            "Step 750/800 | loss_C: -2.4120 | loss_G: -0.3621\n",
            "Step 800/800 | loss_C: -2.6212 | loss_G: -0.2321\n",
            "Training finished (toy run). You can now: - increase `steps` - increase dataset size - add cycle+reward to the generator loss.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For Cell 4**\n",
        "\n",
        "What it did:\n",
        "\n",
        "Defined a Quantum Generator (G): noise â†’ VQC â†’ classical MLP â†’ 450-dim graph vector.\n",
        "\n",
        "Defined a Critic (C): MLP that scores real vs fake graphs.\n",
        "\n",
        "Converted QM9 molecules into fixed-size graph vectors (adjacency + features).\n",
        "\n",
        "Implemented WGAN-GP training loop:\n",
        "\n",
        "Critic tries to separate real/fake graphs.\n",
        "\n",
        "Generator tries to fool the Critic.\n",
        "\n",
        "Printed loss_C (critic loss) and loss_G (generator loss).\n",
        "\n",
        "What you achieved:\n",
        "âœ… You trained your first Hybrid Quantum GAN â€” generator with a quantum circuit vs critic, using real QM9 molecules as training targets."
      ],
      "metadata": {
        "id": "sgSCCCWzwVus"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cell 5 (Phase-5: Add Cycle Consistency)**\n",
        "\n",
        "What it did:\n",
        "\n",
        "Added a Cycle Network (Cyc): graph vector â†’ reconstruct original noise.\n",
        "\n",
        "Added Cycle loss = MSE(z_recon, z).\n",
        "\n",
        "Modified generator objective:\n",
        "\n",
        "ğ¿\n",
        "ğº\n",
        "=\n",
        "âˆ’\n",
        "Critic(fake)\n",
        "+\n",
        "ğœ†\n",
        "cycle\n",
        "â‹…\n",
        "CycleLoss\n",
        "L\n",
        "G\n",
        "\tâ€‹\n",
        "\n",
        "=âˆ’Critic(fake)+Î»\n",
        "cycle\n",
        "\tâ€‹\n",
        "\n",
        "â‹…CycleLoss\n",
        "\n",
        "Training loop now updates Generator + Cycle together.\n",
        "\n",
        "Printed loss_C, loss_G, and cycle_loss.\n",
        "\n",
        "What you achieved:\n",
        "âœ… You upgraded the model to HQ-Cycle-GAN.\n",
        "\n",
        "Generator learns not just to fool the critic, but also to keep its latent noise space consistent (important for stable molecule generation)."
      ],
      "metadata": {
        "id": "dexY3nZ-wdNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Phase 5: Add Cycle Consistency to GAN training =====\n",
        "# Assumes Phase-4 code already ran and G, C, opt_G, opt_C, real_loader, etc. are defined.\n",
        "\n",
        "# ----------------- Cycle Network -----------------\n",
        "class CycleNet(nn.Module):\n",
        "    def __init__(self, in_dim=GRAPH_DIM, out_dim=noise_dim, hidden=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, out_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "Cyc = CycleNet(in_dim=GRAPH_DIM, out_dim=noise_dim).to(device)\n",
        "opt_Cyc = optim.Adam(Cyc.parameters(), lr=lr)\n",
        "\n",
        "lambda_cycle = 10.0   # weight for cycle loss\n",
        "\n",
        "# ----------------- Training loop with Cycle Loss -----------------\n",
        "steps = 500\n",
        "print_every = 50\n",
        "\n",
        "real_iter = iter(real_loader)\n",
        "for step in range(steps):\n",
        "    # ---- Train critic ----\n",
        "    for _ in range(critic_iters):\n",
        "        try:\n",
        "            real_batch = next(real_iter)\n",
        "        except StopIteration:\n",
        "            real_iter = iter(real_loader)\n",
        "            real_batch = next(real_iter)\n",
        "        real_batch = real_batch.to(device)\n",
        "\n",
        "        z = torch.randn(batch_size, noise_dim, device=device).float()\n",
        "        with torch.no_grad():\n",
        "            fake_batch = G(z)\n",
        "\n",
        "        real_scores = C(real_batch)\n",
        "        fake_scores = C(fake_batch)\n",
        "        gp = gradient_penalty(C, real_batch, fake_batch, device)\n",
        "\n",
        "        loss_C = fake_scores.mean() - real_scores.mean() + lambda_gp * gp\n",
        "\n",
        "        opt_C.zero_grad()\n",
        "        loss_C.backward()\n",
        "        opt_C.step()\n",
        "\n",
        "    # ---- Train generator + cycle ----\n",
        "    z = torch.randn(batch_size, noise_dim, device=device).float()\n",
        "    fake = G(z)\n",
        "\n",
        "    # cycle consistency\n",
        "    z_recon = Cyc(fake)\n",
        "    cycle_loss = F.mse_loss(z_recon, z)\n",
        "\n",
        "    # generator loss (WGAN + cycle)\n",
        "    loss_G = -C(fake).mean() + lambda_cycle * cycle_loss\n",
        "\n",
        "    opt_G.zero_grad()\n",
        "    opt_Cyc.zero_grad()\n",
        "    loss_G.backward()\n",
        "    opt_G.step()\n",
        "    opt_Cyc.step()\n",
        "\n",
        "    if (step+1) % print_every == 0 or step==0:\n",
        "        print(f\"Step {step+1}/{steps} | loss_C: {loss_C.item():.4f} | loss_G: {loss_G.item():.4f} | cycle_loss: {cycle_loss.item():.4f}\")\n",
        "\n",
        "print(\"Phase 5 training finished âœ… (GAN + Cycle).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qp6vzZQhofXJ",
        "outputId": "bf1dbe87-cb42-404b-ba9e-bbf74fc82518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1/500 | loss_C: -2.5160 | loss_G: 9.0162 | cycle_loss: 0.9345\n",
            "Step 50/500 | loss_C: -2.5529 | loss_G: 8.3670 | cycle_loss: 0.8638\n",
            "Step 100/500 | loss_C: -2.4494 | loss_G: 10.8148 | cycle_loss: 1.0981\n",
            "Step 150/500 | loss_C: -2.5385 | loss_G: 9.5043 | cycle_loss: 0.9711\n",
            "Step 200/500 | loss_C: -2.3893 | loss_G: 9.1311 | cycle_loss: 0.9290\n",
            "Step 250/500 | loss_C: -2.1534 | loss_G: 10.0306 | cycle_loss: 1.0356\n",
            "Step 300/500 | loss_C: -2.2409 | loss_G: 9.3586 | cycle_loss: 0.9608\n",
            "Step 350/500 | loss_C: -2.2464 | loss_G: 8.7382 | cycle_loss: 0.8948\n",
            "Step 400/500 | loss_C: -2.1547 | loss_G: 9.3212 | cycle_loss: 0.9718\n",
            "Step 450/500 | loss_C: -2.1129 | loss_G: 6.4219 | cycle_loss: 0.6667\n",
            "Step 500/500 | loss_C: -2.0063 | loss_G: 9.3943 | cycle_loss: 0.9812\n",
            "Phase 5 training finished âœ… (GAN + Cycle).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The original paperâ€™s HQ-Cycle-MolGAN has 3 parts in the objective:\n",
        "\n",
        "WGAN loss â†’ adversarial training (âœ… done in Cell 4).\n",
        "\n",
        "Cycle loss â†’ consistency between latent space and molecules (âœ… done in Cell 5).\n",
        "\n",
        "Reward loss â†’ optimize molecules for chemical properties like QED, LogP, SA (âŒ not yet added)."
      ],
      "metadata": {
        "id": "-qrHRC2pwojI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Phase 6: Reward Integration without RDKit =====\n",
        "# Assumes Phase-5 (CycleNet, G, C, etc.) has already run.\n",
        "\n",
        "def compute_synthetic_reward(fake_batch):\n",
        "    \"\"\"\n",
        "    Fake reward signal as a placeholder for RDKit properties.\n",
        "    Measures variance of adjacency part of graph vector.\n",
        "    Higher variance => higher reward.\n",
        "    \"\"\"\n",
        "    adj = fake_batch[:, :GRAPH_ADJ_FLAT]   # first 405 dims = adjacency\n",
        "    reward = adj.var(dim=1)                # variance across edges\n",
        "    reward = torch.sigmoid(reward)         # squash into (0,1)\n",
        "    return reward\n",
        "\n",
        "gamma_reward = 5.0\n",
        "steps = 300\n",
        "print_every = 50\n",
        "\n",
        "real_iter = iter(real_loader)\n",
        "for step in range(steps):\n",
        "    # ---- Train Critic ----\n",
        "    for _ in range(critic_iters):\n",
        "        try:\n",
        "            real_batch = next(real_iter)\n",
        "        except StopIteration:\n",
        "            real_iter = iter(real_loader)\n",
        "            real_batch = next(real_iter)\n",
        "        real_batch = real_batch.to(device)\n",
        "\n",
        "        z = torch.randn(batch_size, noise_dim, device=device).float()\n",
        "        with torch.no_grad():\n",
        "            fake_batch = G(z)\n",
        "\n",
        "        real_scores = C(real_batch)\n",
        "        fake_scores = C(fake_batch)\n",
        "        gp = gradient_penalty(C, real_batch, fake_batch, device)\n",
        "\n",
        "        loss_C = fake_scores.mean() - real_scores.mean() + lambda_gp * gp\n",
        "        opt_C.zero_grad()\n",
        "        loss_C.backward()\n",
        "        opt_C.step()\n",
        "\n",
        "    # ---- Train Generator + Cycle + Reward ----\n",
        "    z = torch.randn(batch_size, noise_dim, device=device).float()\n",
        "    fake = G(z)\n",
        "\n",
        "    # cycle loss\n",
        "    z_recon = Cyc(fake)\n",
        "    cycle_loss = F.mse_loss(z_recon, z)\n",
        "\n",
        "    # synthetic reward\n",
        "    reward_vals = compute_synthetic_reward(fake)\n",
        "    reward_loss = -reward_vals.mean()   # minus: maximize reward\n",
        "\n",
        "    # total generator loss\n",
        "    loss_G = -C(fake).mean() + lambda_cycle * cycle_loss + gamma_reward * reward_loss\n",
        "\n",
        "    opt_G.zero_grad()\n",
        "    opt_Cyc.zero_grad()\n",
        "    loss_G.backward()\n",
        "    opt_G.step()\n",
        "    opt_Cyc.step()\n",
        "\n",
        "    if (step+1) % print_every == 0 or step==0:\n",
        "        print(f\"Step {step+1}/{steps} | loss_C: {loss_C.item():.4f} \"\n",
        "              f\"| loss_G: {loss_G.item():.4f} \"\n",
        "              f\"| cycle_loss: {cycle_loss.item():.4f} \"\n",
        "              f\"| reward: {reward_vals.mean().item():.4f}\")\n",
        "\n",
        "print(\"âœ… Phase-6 training finished: HQ-Cycle-MolGAN (Adversarial + Cycle + Synthetic Reward).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zY3_sEMmgIy",
        "outputId": "c558509b-0513-4fd0-8cb4-062e65dd0b9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1/300 | loss_C: -2.2216 | loss_G: 8.7303 | cycle_loss: 1.1642 | reward: 0.5077\n",
            "Step 50/300 | loss_C: -2.1954 | loss_G: 5.9414 | cycle_loss: 0.8731 | reward: 0.5077\n",
            "Step 100/300 | loss_C: -2.0074 | loss_G: 5.0709 | cycle_loss: 0.8023 | reward: 0.5080\n",
            "Step 150/300 | loss_C: -1.8554 | loss_G: 9.1027 | cycle_loss: 1.2062 | reward: 0.5086\n",
            "Step 200/300 | loss_C: -2.0954 | loss_G: 6.3778 | cycle_loss: 0.9082 | reward: 0.5083\n",
            "Step 250/300 | loss_C: -1.6971 | loss_G: 5.9575 | cycle_loss: 0.8656 | reward: 0.5081\n",
            "Step 300/300 | loss_C: -1.9689 | loss_G: 5.9378 | cycle_loss: 0.8674 | reward: 0.5086\n",
            "âœ… Phase-6 training finished: HQ-Cycle-MolGAN (Adversarial + Cycle + Synthetic Reward).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Why We dont get the Diagrams -\n",
        "1.We havenâ€™t implemented the decoder (fake graph vector â†’ RDKit molecule).\n",
        "\n",
        "2.And since RDKit wasnâ€™t installed, we avoided molecule visualization altogether.\n",
        "\n",
        "##ğŸ”¹ Without RDKit, can we still visualize molecules?\n",
        "\n",
        "Strictly speaking â€” we canâ€™t render chemical structures (atoms/bonds) without a cheminformatics toolkit like RDKit, OpenBabel, or ChemAxon. These libraries know the rules of valence, atom symbols, bond types, etc.\n",
        "\n",
        "But â€” since your Generator already outputs adjacency (9Ã—9Ã—5) and features (9Ã—5), you can visualize these as graph diagrams instead of â€œchemicalâ€ diagrams.\n",
        "\n",
        "##ğŸ”¹ Alternative Visualization Methods (no RDKit)\n",
        "\n",
        "NetworkX Graph Visualization\n",
        "\n",
        "**1.Treat atoms as nodes, bonds as edges.**\n",
        "```\n",
        "Node color = atom type (C, O, N, etc.)\n",
        "\n",
        "Edge style = bond type (single, double, triple).\n",
        "\n",
        "Use networkx.draw + matplotlib.\n",
        "\n",
        "ğŸ‘‰ You wonâ€™t get perfect chemistry (valence checks, aromaticity, etc.), but youâ€™ll get a graph-like diagram showing how the Generator connects nodes.\n",
        "```\n",
        "\n",
        "**2.Heatmaps**\n",
        "```\n",
        "Plot adjacency tensor slices (each bond type).\n",
        "\n",
        "Each is a 9Ã—9 matrix â€” heatmap shows how strongly the Generator predicted each edge.\n",
        "\n",
        "Atom features (9Ã—5) can be plotted as a color-coded matrix.\n",
        "```\n",
        "\n",
        "**3.Graph Embedding Visualization**\n",
        "```\n",
        "Project the generated graph vectors into 2D (via PCA or t-SNE).\n",
        "\n",
        "Plot them alongside real QM9 vectors.\n",
        "\n",
        "This shows whether Generatorâ€™s outputs â€œlook likeâ€ real molecules, even without drawing molecules.\n",
        "```"
      ],
      "metadata": {
        "id": "-5TE666ftZt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here We are Implementing the Visualisation Part without RDkit\n",
        "# We will use  networkx  and matplotlib\n",
        "\n",
        "!pip install networkx matplotlib\n",
        "\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_graph(vec):\n",
        "    # Split vector into adjacency (9x9x5) and features (9x5)\n",
        "    adj = vec[:GRAPH_ADJ_FLAT].reshape(9,9,5)\n",
        "    feats = vec[GRAPH_ADJ_FLAT:].reshape(9,5)\n",
        "\n",
        "    # Pick most likely atom for each node\n",
        "    atom_ids = feats.argmax(axis=1)\n",
        "    atom_labels = [ATOM_TYPES[i] if i < len(ATOM_TYPES) else \"X\" for i in atom_ids]\n",
        "\n",
        "    # Create graph\n",
        "    G = nx.Graph()\n",
        "    for i, label in enumerate(atom_labels):\n",
        "        if feats[i].max() > 0.5:  # keep active nodes\n",
        "            G.add_node(i, label=label)\n",
        "\n",
        "    bond_types = [\"-\", \"=\", \"#\", \"ar\", \"other\"]\n",
        "    for i in range(9):\n",
        "        for j in range(i+1,9):\n",
        "            bt = adj[i,j].argmax()\n",
        "            if adj[i,j].max() > 0.5:  # threshold\n",
        "                G.add_edge(i, j, bond=bond_types[bt])\n",
        "\n",
        "    # Draw\n",
        "    labels = nx.get_node_attributes(G, 'label')\n",
        "    edge_labels = nx.get_edge_attributes(G, 'bond')\n",
        "    pos = nx.spring_layout(G)\n",
        "    nx.draw(G, pos, labels=labels, with_labels=True, node_color=\"lightblue\", node_size=800)\n",
        "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
        "    plt.show()\n",
        "\n",
        "# Example: visualize one fake molecule\n",
        "z = torch.randn(1, noise_dim, device=device).float()\n",
        "fake_vec = G(z).detach().cpu().numpy()[0]\n",
        "visualize_graph(fake_vec)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        },
        "id": "5q-bj9VSvE03",
        "outputId": "8627430d-54f0-4ce9-b6f6-b20148f09d0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (3.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#This Ends the Implementation of MolGAN Research Paper\n",
        "\n",
        "**To do Further\n",
        "```\n",
        "Do Parrallel Programing\n",
        "1.Study Multiplication and addition code of cuda , and apply in the program above .\n",
        "```"
      ],
      "metadata": {
        "id": "fppmTc6svibw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CGxKMlPTYWjV"
      }
    }
  ]
}